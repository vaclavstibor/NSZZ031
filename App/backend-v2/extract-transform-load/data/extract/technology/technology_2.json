[
    {
        "id": "35c23749-b326-43b0-9baf-1cc0a4d473b1",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/article/2024/may/02/apple-earnings-iphone-sales-decrease",
        "title": "Apple reports slumping iPhone sales as global demand weakens",
        "content": "Apple released its earnings report on Thursday, revealing a drop in overall revenue fueled by slackening iPhone sales. Earnings exceeded market expectations, however, and Apple’s shares rose in after-hours trading. Tim Cook, Apple’s chief executive, said in a statement released before the call that “Apple is reporting revenue of $90.8bn for the March quarter, including an all-time revenue record in services”. The iPhone manufacturer reported revenue of $90.8bn, down 4% year-over-year, but surpassing anticipated earnings of $90.1bn. It declared $0.25 in cash dividend for each share, an increase of 4%. iPhone revenue was $45.96bn, down 10% from the same time period last year, whereas Wall Street’s estimations were $46bn. The company also reported that its board had authorized a $110bn stock buyback. “Longer term, I think that Apple’s shift to a service business model is a robust approach to compensate for its dependence on iPhone sales performance,” said Forrester vice-president and principal analyst Thomas Husson. Apple unseated Samsung late last year as the world’s largest smartphone provider, capturing about 20% of the global market share. Shipments of Apple’s smartphones declined in the first quarter of this year, however, as Chinese competitors such as Huawei gained ground. Samsung, which saw a lighter fall in its shipments than Apple, took back the top spot among smartphone makers in the first months of 2024. During the Q&amp;A portion of the earnings call, Cook and Apple’s chief financial officer, Luca Maestri, fielded multiple questions about the company’s performance in China. Demand in the country has fallen, although the earnings report showed that the situation was less dire than many investors feared. Cook also said Apple would make “significant investments” in generative AI in the coming quarter, in line with similar announcements from Amazon, Alphabet, Microsoft and Meta in recent weeks. He repeatedly brought up artificial intelligence, claiming that the MacBook Air was the “best consumer laptop for AI”. “We continue to feel very bullish about our opportunity in generative AI,” Cook said on the call. “We are making significant investments and we’re looking forward to sharing some very exciting things with our customers soon.” As with many big tech companies over the past year, Apple made cuts in several departments, laying off a sizable number of employees. It shuttered a decade-long project in February to produce an autonomous electric car and laid off about 600 workers last month following the announcement. It is estimated the development of the car, codenamed Project Titan, cost Apple roughly $10bn. The company’s electric vehicle operation had a long history of turmoil, but the full shutdown of the multibillion-dollar effort surprised both the public and Apple employees working on the project. When the electric vehicle project was canceled, Apple executives also announced that they would be dedicating more resources to artificial intelligence projects. Apple has been poaching AI researchers from other tech companies such as Google, according to a Financial Times analysis, and is quietly setting up an AI research lab in Zurich. Apple has also been releasing AI research papers that may predict new features in its phones. Apple has been less public about its foray into generative AI than rivals like Microsoft and Google, but has similarly poured money into staffing up and acquiring AI startups. It is expected to reveal more of its plans for integrating generative AI into smartphones later this year. The Vision Pro headset, Apple’s first new gadget in a decade, was released in early February. According to its two most recent earnings reports, the product does not make a significant contribution to Apple’s revenue. The company is also facing numerous legal battles in the coming months, as regulators in the US and Europe have issued fines and levied antitrust lawsuits. The US Department of Justice filed a landmark antitrust lawsuit in March, accusing Apple of engaging in “a broad, sustained, and illegal course of conduct” to establish a smartphone monopoly. “Apple creates barriers and makes it extremely difficult and expensive for both users and developers to venture outside the Apple ecosystem,” Merrick Garland, the attorney general, said while announcing the suit. European authorities also fined Apple €1.8bn for EU antitrust violations, alleging that the company blocked competition from other music streaming services through unnecessary restrictions on its app store. Apple has rejected the allegations from European and US regulators, and vowed that it will contest the cases against its business practices. Apple’s stock price declined in recent months to fluctuate around $170 per share, down from its all-time high of about $199 which it hit last December. Shares of Apple rose in the days before the earnings report after a well-known financial analyst at Bernstein upgraded the stock to a prediction that it would outperform current market expectations and advised people to buy Apple stock. Cook ended his part of the call touting Apple’s commitment to environmentally friendly operations, and putting a positive spin on the company’s forthcoming products. “I couldn’t be more excited for the future we have ahead of us, driven by the imagination and innovation of our teams and the enduring importance of our products and services in people’s lives,” Cook said.",
        "author": "Nick Robins-Early",
        "published_date": "2024-05-02T22:18:39+00:00"
    },
    {
        "id": "48e211bd-0c3d-4fb4-8c4f-8d99f34d30d3",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/article/2024/may/02/apple-working-to-fix-iphone-alarm-problem",
        "title": "Apple working to fix iPhone alarm problem",
        "content": "Apple is working to fix a problem that has resulted in some users complaining that their iPhone alarms are not going off – or playing too quietly. The company said it was aware of the issue, which has been picked up by TikTok users, who have complained about incidents where their alarm has failed to sound. “This has probably been the third or fourth day in a row that my alarm clock has not gone off,” said one TikTok user. Another said: “I’ve noticed for the past week or so my alarm just wasn’t waking me up.” Users said the problem was causing them timekeeping problems. Apple said it was working to fix the problem quickly, although it is not clear how many people have been affected or what devices are involved. Another TikTok user, in a video that has garnered nearly 10m views, said their iPhone 15 alarm had gone off at “the lowest volume” and claimed the problem was related to an “attention aware” feature being enabled on their phone. “Apple are you trying to get people fired?” the user added. Apple declined to comment on whether the feature was the reason for the alarm problem, which was first reported by NBC’s Today programme. The attention aware feature lowers the sound of alerts if users are looking at their device. If you are looking at your phone it also dims the display until you stop looking at it. Apple has an online advice page on alarms, which points to controlling the alarm volume via the “Sounds &amp; Haptics” feature under settings, adding that the “do not disturb” and “silent mode” options do not affect the alarm sound. The page also recommends that users check their alarm sound is not set to “none”.",
        "author": "Dan Milmo",
        "published_date": "2024-05-02T12:18:56+00:00"
    },
    {
        "id": "5917c7fa-790d-401a-bdfc-550f2b404cca",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/may/01/chatgpt-chatbot-rival-claude-to-be-introduced-on-iphone",
        "title": "ChatGPT’s chatbot rival Claude to be introduced on iPhone",
        "content": "OpenAI’s ChatGPT is facing serious competition, as the company’s rival Anthropic brings its Claude chatbot to iPhones. Anthropic, led by a group of former OpenAI staff who quit over differences with chief executive Sam Altman, have a product that already beats ChatGPT on some measures of intelligence, and now wants to win over everyday users. “In today’s world, smartphones are at the centre of how people interact with technology. To make Claude a true AI assistant, it’s crucial that we meet users where they are – and in many cases, that’s on their mobile devices,” said Scott White at Anthropic. “We’re putting the power of Claude directly into people’s hands. It’s not just about convenience; it’s about integrating Claude into the fabric of our daily lives.” The third version of the Claude large language model is offered direct to users on its website in three flavours: a speedy and simple model called “haiku”, a slower and more powerful model called “sonnet”, and, for paying customers only, the full “opus” system. It is that system that took the lead in the LMSys chatbot ranking, becoming the first AI to knock GPT-4 out of pole position, and it also made headlines for its enormous “context window” – a measure of how much of a conversation it can keep in mind at any one time. Opus can hold about 160,000 words, enough for a user to paste in a weighty novel and ask follow-up questions. Until now, though, ChatGPT has faced little competition on users’ devices. OpenAI first released its iOS app in May last year, and it remains one of the few frontier AI models with an accessible consumer app. Anthropic says the Claude app will allow it to bring new features to users, beyond simple ease of use. “For example, the Claude iOS app can, with a user’s consent, access the device’s camera and photo library,” White said. “After a meeting, a business user could snap a photo of a whiteboard diagram and ask Claude to summarise the key points, making it easier to share and act upon important information. Similarly, a consumer could take a picture of a plant they encounter on a hike and ask Claude to identify the species and provide more information about its characteristics and habitat.” Alongside the iOS app, the company is also boosting its business offering, with a “team” plan that lets corporate customers buy chatbot access for their entire staff. “We started Anthropic to lead the frontier of AI safety and research. That isn’t something you can do in the abstract. We don’t think we’d be able to positively influence the industry’s trajectory and inspire a race to the top on AI safety if we weren’t able to compete at the frontier,” White said. That competition appears to be having an effect on the market leader. On Wednesday, OpenAI changed its policies to allow users to access their entire ChatGPT history, without needing to opt in to allowing the company to train on their conversations as a quid pro quo.",
        "author": "Alex Hern",
        "published_date": "2024-05-01T15:00:33+00:00"
    },
    {
        "id": "60c20fba-cafe-4b6a-8c7d-300065051a69",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/30/amazon-sales-report-ai",
        "title": "Amazon sales soar with boost from artificial intelligence and advertising",
        "content": "Amazon profits soared once again in the first quarter of 2024, the company announced on Tuesday – the latest in a series of robust earnings reports for the retail giant. The company attributed the boost to artificial intelligence and advertising sales. Amazon reported overall revenue of $143.3bn in the first three months of the year – up 13% from the same period in 2023 and surpassing Wall Street expectations of $142.65bn. The e-commerce giant reported an increase of more than 200% to $15bn, with net income more than tripling to $10.4bn from $3.17bn at the same time in 2023. In a statement accompanying the report, the chief executive, Andy Jassy, said Amazon’s continuing focus on AI has “reaccelerat[ed]” the growth rate of Amazon Web Services (AWS), the company’s cloud-computing sector. Revenue at AWS increased 17% year-over-year to $25bn, and AWS accounted for 62% of total operating profit. In a call following the report, Jassy said Amazon still has a lot of room to grow in the generative AI sector. The boost to AWS comes after growth in the sector had slowed recently. Executives attributed the slump to recovery from the Covid-19 pandemic, which had pushed many companies to improve cloud infrastructure to support remote work. That trend is stabilizing, they said, and demand for AI can further boost its cloud services. “We remain very bullish in AWS,” Jassy said. “We’re at a $100bn-dollar annualized revenue run right now, and this is before you even calculate generative AI. There’s a very large opportunity in front of us.” Advertising sales, meanwhile, increased 24% year-over-year to $11.8bn, after the company expanded its advertising, including rolling out ads on Prime Video, starting earlier this year. As Amazon ramps up its cloud-computing and AI capabilities, it will need to spend more on infrastructure to support the technology, Jassy said on the investor call Tuesday. Capital expenditure (capex) was at $14bn for the quarter, and Jassy said it is expected to increase in subsequent quarters of the fiscal year. “The more demand AWS has, the more we have to procure new data centers’ power and hardware,” he said, adding that the company does not spend capital “without very clear signals that we can monetize it”. Last week, Meta’s shares plummeted on news that the company would increase its capital expenditure to build up its AI capabilities. The earnings report comes after Amazon announced it would invest $11bn to build more data centers in Indiana, promising at least 1,000 jobs there. Also in the quarter, the company extended its partnership with chip manufacturer Nvidia to continue to power its AI offerings. The report underscores a positive response from investors to Amazon’s recent cost-cutting measures, including laying off more than 27,000 employees since late 2022. Amazon laid off hundreds of additional staffers in early 2024. Shares were up 5% in after-hours trading.",
        "author": "Kari Paul",
        "published_date": "2024-04-30T22:47:44+00:00"
    },
    {
        "id": "6e331041-031d-4c51-ae11-16da7c5d1c5d",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/30/binance-founder-sentenced-money-laundering",
        "title": "Binance founder sentenced to four months for money laundering",
        "content": "Changpeng Zhao, the former head of the world’s largest cryptocurrency trading company, was sentenced to four months in jail on Tuesday in a Seattle courtroom. Zhao pleaded guilty late last year to money-laundering violations and stepped down as CEO of Binance. The company itself was fined $4.3bn. Zhao was fined $50m last year. Judge Richard Jones told Zhao that there were a number of mitigating factors in his sentencing, including that he had cooperated with law enforcement. Jones also cited numerous letters the court had received that testified to Zhao’s character, and stated that he did not believe Zhao was likely to reoffend. The Department of Justice sought a 36-month sentence for Zhao in a filing last week, arguing that he “violated US law on an unprecedented scale” and that his “sentence should reflect the gravity of his crimes”. Zhao’s defense lawyers argued that he should receive only probation. “Mr Zhao deeply regrets his offense, and he has shown exceptional acceptance of responsibility and remediation,” Zhao’s lawyers said in their filing before the sentencing. Zhao, who goes by CZ, has for years been one of the biggest names in cryptocurrency. He amassed billions of dollars as head of Binance, founding the exchange in 2017 and growing it into a dominant industry player as crypto investments surged. Zhao’s downfall coincided with a broader legal scrutiny of the crypto industry, including his fellow billionaire Sam Bankman-Fried being sentenced to 25 years in prison this year for financial fraud. The justice department’s case against Zhao revolved around his failure to comply with US anti-money-laundering laws and to file proper reports with government agencies such as the Financial Crimes Enforcement Network. In practice, prosecutors say that led to Binance becoming a hub for illicit financial transactions that included extremist groups, criminals and people trafficking child sexual abuse materials. Binance failed to report more than 100,000 suspicious transactions to law enforcement, prosecutors alleged, including those involving US-designated terror groups such as al-Qaida, Islamic State and Hamas. “From the very beginning, Zhao and other Binance executives had engaged in a deliberate and calculated effort to profit from the US market without implementing the controls that are required by US law,” the attorney general, Merrick Garland, said after Zhao pleaded guilty. During the sentencing hearing, the probation department recommended that Zhao receive five months in prison. They emphasized trying to find a balance between a deterrent and something that would not be unusually harsh. Zhao’s lawyers argued that the government seeking three years in prison was out of step with similar cases where defendants received no jail time. Zhao issued a short statement during the hearing, telling the judge that he wanted to focus his efforts on an online education platform and that he had reflected on his actions. Zhao remained active in the lead-up to the sentencing, traveling around the country and once meeting with Sam Altman, the OpenAI CEO. Earlier this year, he announced to his nearly 9 million followers on the social media platform X that he was starting an online education program, an effort that he brought up again in his remarks at the hearing. In addition to the sentence, Zhao’s plea deal requires him to pay $50m personally and Binance to pay $4.3bn in fines. The deal also carries potential US immigration implications for Zhao, who is a Canadian and Emirati citizen, but who is known for frequently moving between cities. Prosecutors argued following his plea deal that Zhao was a flight risk due to his immense wealth, and a judge ruled that he had to remain in the US awaiting sentencing. Jones stated in the sentencing hearing that Zhao should understand that wealth and power do not make a person immune from legal consequences, adding that Binance had turned a blind eye to criminal activity on its platform. Few within crypto ever approached Zhao’s status within the industry. While Zhao’s former rival Bankman-Fried became a public face and courted politicians in Washington, Zhao’s wealth and influence over crypto far surpassed him. Binance also played an important role in the downfall of Bankman-Fried’s FTX exchange, after Zhao’s decision in 2022 to liquidate Binance’s holdings of FTX’s own cryptocurrency accelerated investor concerns and hastened the rival exchange’s implosion.",
        "author": "Nick Robins-Early",
        "published_date": "2024-04-30T19:20:41+00:00"
    },
    {
        "id": "ddff7178-336f-4c34-82f0-274d9c33ddce",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/30/eu-meta-russian-interference-elections-facebook-fake-news",
        "title": "Fears of Putin swinging elections behind EU’s Meta crackdown",
        "content": "Fears that Vladimir Putin is trying to fill the European parliament with more pro-Russia MEPs were behind the EU’s blunt message to the Silicon Valley owner of Facebook on Tuesday. It gave Meta just five days to explain how it will root out fake news, fake websites and stop adverts funded by the Kremlin or face severe measures. Forty days out from the European parliamentary elections – and during a year in which countries with more than half the world’s population go to the polls – deep concerns about how Facebook is dealing with fake news were behind the warning. “The integrity of the election is an enforcement priority,” said Thierry Breton, the commissioner for internal market, warning that the European Commission would be quick to respond if Facebook did not rectify the problems within the week. “We expect Meta to inform us of the actions they are taking to address these risks in five working days or we will take all necessary measures to defend our democracy,” he said. The commission confirmed it had launched formal proceedings against Meta as the clock ticks down to elections being held across Europe on 6-9 June. The commission is extremely concerned that Russia will use Facebook, which has more than 250 million monthly active users, to try to swing votes in its favour. As the Belgian prime minister, Alexander De Croo, said earlier this month, after a formal investigation into alleged payments by the Kremlin to MEPs, the objective of Russia is “very clear”: to help “elect more pro-Russian candidates in the European parliament”. Officials declined to give precise examples but some are blatant, including adverts paid for by foreign agents. “It is fundamentally wrong they [Facebook] are making money on this,” said an official. They also say the tools to flag illegal or suspicious content are not visible enough. Links to fake news platforms, known as “doppelganger sites”, are not being removed quickly enough or at all, the EU suggests. Last week a Czech news agency website was hacked to display fake news including claims that an assassination attempt on the Slovak president had been foiled. At the same time France’s Europe minister, Jean-Noël Barrot, said the country was being “pounded” by Russian propaganda with “deliberate manoeuvres to disrupt public debate and interfere in the campaign for the European elections”. Another concern on Facebook is Meta’s decision to “suppress” discourse in an effort to de-risk user-generated content on sensitive subjects such as the Middle East. This is known as “shadow banning”, and the EU wants Facebook to be more transparent in how it justifies these decisions. “Users need to know about it when it happens and they need to be able to appeal it, otherwise this is a discourse risk,” said an official. It is also concerned that Facebook was planning to discontinue a service called CrowdTangle that helped factcheckers, journalists and researchers to monitor disinformation. Tuesday’s proceedings against Facebook are the sixth taken by the European Commission since the EU’s Digital Services Act (DSA) came into force. But is it enough to stop the lies? Even officials in Nato, on a panel in Brussels in February, said they were treating disinformation as potent a weapon as bullets and missiles. Officials say it is not that Facebook is “not doing anything”, it is just that the measures in place are weak, opaque and not effective enough. Under sweeping new laws under the DSA, which came into force in August, the EU can fine social media companies up to 6% of their revenue or ban them from the union altogether. Facebook said: “We have a well-established process for identifying and mitigating risks on our platforms. We look forward to continuing our cooperation with the European Commission and providing them with further details of this work.”",
        "author": "Lisa O'Carroll",
        "published_date": "2024-04-30T11:42:53+00:00"
    },
    {
        "id": "8a8e5526-8a57-49df-af3c-9e251c29d1fa",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/30/techscape-artificial-intelligence-bots-dead-internet-theory",
        "title": "TechScape: On the internet, where does the line between person end and bot begin?",
        "content": "I know I’m real. And you, dear reader, know you’re real. But do you ever suspect that everyone else on the internet is acting strange? That the spaces you used to frequent feel a bit … dead? You aren’t alone. “Dead internet theory” first hit the web almost three years ago, propelled to the mainstream by an essay in the Atlantic by Kaitlyn Tiffany: Dead-internet theory suggests that the internet has been almost entirely taken over by artificial intelligence. Like lots of other online conspiracy theories, the audience for this one is growing because of discussion led by a mix of true believers, sarcastic trolls and idly curious lovers of chitchat … But unlike lots of other online conspiracy theories, this one has a morsel of truth to it. Person or bot: Does it really matter? At the time it was writing, the deadest part of the internet was Twitter in its moribund pre-Musk years. The site’s aggressive curation served up the same “relatable content” to hundreds of thousands of users, who made tweaks to posts like “i hate texting come over and cuddle me” and then reposted them. The distinction between person and bot was also being blurred by a recommendation algorithm that drove people to act like bots. Beyond that central idea, the 2021 version of the conspiracy theory took some weird turns. One proponent, Tiffany wrote, “suggests that the internet died in 2016 or early 2017, and that now it is ‘empty and devoid of people,’ as well as ‘entirely sterile’… As evidence, IlluminatiPirate offers, ‘I’ve seen the same threads, the same pics, and the same replies reposted over and over across the years.’” The theory wasn’t wrong – it was just too soon. Talking about a dead internet the summer before the release of ChatGPT is like the Guardian colleague who confidently declared, in the summer of 2016: “It’s been a mad rush of political news since the 2014 local elections, so it’s weird to think that there’s just Brexit and the US election and then everything will be quiet for the next few years.” In 2021, the internet felt dead because aggressive algorithmic curation was driving people to act like robots. In 2024, the opposite has happened: the robots are posting like people. Here are just a few examples: On Twitter itself, after Musk rescued the site from the frying pan and tossed it in a volcano, an ill-conceived monetisation scheme has made it profitable to buy a blue checkmark, attach it to a large language model, and set it running wild replying to viral content. The social network now pays verified users a proportion of the ad revenue received from their own comment threads, turning the most viral posts on the site into a low-stakes all-bot battle royale. Death permeates Google. The top of its search results is a valuable position – so valuable that businesses competing to be there have no spare money to actually write their articles. No problem: ChatGPT can churn something out in a second. Of course, that’s only valuable if the resultant visitors are humans who you can make money from. Bad news, because … … across the web, bots account for around half of all internet traffic, according to research from cybersecurity firm Imperva. Almost one-third of all traffic is what the company calls “bad bots”, doing anything from ad fraud to brute force hacking attacks. But even the “good bots” are struggling to earn that categorisation: Google’s “crawler” was a welcome sight when it was updating your search entry, but less so when it was simply training an AI to repeat what you wrote without sending any users over. And then there’s Crab Jesus. An unholy marriage of Facebook content farms, AI-generated imagery, and automated testing to work out what goes the most viral led to weeks of viral content featuring combinations of Jesus, crustaceans and female flight attendants. In one such image, Jesus was pictured eating shellfish wearing a jacket made of prawns. More confusing was the image of a sort of crab-centaur saviour walking along a beach arm-in-arm with what appears to be the entire crew from a long-haul flight. It was, at least, interestingly bizarre – a step up from the previous viral chum of the 122-year-old woman posing in front of her homemade birthday cake. I want to provide a ray of hope here, a nice little tip for how to insert some vitality into the internet again, but I can’t. It really does feel as if the public-facing net is in the latter stages of a zombie apocalypse. The good news is that there are safe havens. “Private social”, like WhatsApp and Discord servers, can hide from the onslaught in secrecy, while smaller communities such as Bluesky and Mastodon are safe through obscurity, for now. In the mid-term, I expect to see large platforms cotton on to the wasteland their services have become, and use a combination of account verification and AI detection to try and restore some humanity to their offerings. Whether it will be too late by then, though, is an open question. Musk still needs his Twitter sitter There is, at least, one human still on the internet: Elon Musk, who is so addicted to posting that he spent $44bn to be called an idiot on a platform he owns. So his latest legal defeat will have hit him where it hurts, after the US supreme court declined to take up his plea to be freed from a court-appointed posting babysitter. From our story: The supreme court on Monday rejected an appeal from Elon Musk over a settlement with securities regulators that requires him to get approval in advance of some tweets that relate to Tesla, the electric vehicle company he leads. The justices did not comment in leaving in place lower-court rulings against Musk, who complained that the requirement amounts to “prior restraint” on his speech in violation of the first amendment. The ruling comes a day after he made an unannounced visit to China aimed at sealing a deal to roll out Tesla’s driver assistance features there. For those without an encyclopaedic memory of all things Elon: in 2018, Musk tweeted that he had “funding secured” to take Tesla private. The company never went private, and in ensuing lawsuits it turned out that he had, at best, a few discussions about it. To put an end to the legislation, Musk agreed to resign as Tesla chairman, pay $20m and have an in-house lawyer pre-approve all his social media posts about the electric carmaker. He’s regretted it ever since, battling to have that part of the agreement (which he entered in to voluntarily, to avoid a damaging court case) overturned. “The pre-approval provision at issue continues to cast an unconstitutional chill over Mr Musk’s speech whenever he considers making public communications,” his lawyers argued. Well, the supreme court in the US doesn’t care; it didn’t take his case, implicitly determining that no real constitutional issue is at stake. The weird thing is that the in-house lawyer already seems to be taking a very hands-off approach to Musk’s posts. On Friday, he responded to an allegation from early Facebook employee Dustin Moskovitz that Tesla was “the next Enron” by posting a picture of a dog draping its testicles over the face of another dog. (Click at your own risk.) If that’s Musk tweeting with an “unconstitutional chill”, I’d hate to know what he’d send if he felt truly free. If you want to read the complete version of the newsletter please subscribe to receive TechScape in your inbox every Tuesday.",
        "author": "Alex Hern",
        "published_date": "2024-04-30T10:41:27+00:00"
    },
    {
        "id": "12481736-b42f-4de6-aa44-6104a94fdfa0",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/30/smartphones-ban-more-harm-than-good-molly-russell-father",
        "title": "Smartphones ban may cause more harm than good, says Molly Russell’s father",
        "content": "Government proposals to ban the sale of smartphones to under-16s and raise the minimum age for accessing social media risk causing more harm than good, the father of Molly Russell has warned. Ian Russell said it was “no surprise” there is a groundswell of pressure for tougher regulation of social media platforms but said plans for a fresh crackdown were flawed. Writing in the Guardian, Russell said poorly thought-out policies could have unintended consequences. “By rushing to introduce new measures that may sound attractive but that research has shown may be deeply flawed, my fear is that there is a real risk the government introduces a set of poorly thought-out measures that result in multiple unintended consequences. Put simply, much of what is being suggested may cause more harm than good,” he said. Russell has become a prominent voice in the debate over online safety after the death of his 14-year-old daughter, Molly, who killed herself in November 2017 after viewing large amounts of content related to suicide, depression, self-harm and anxiety on Instagram and Pinterest. In 2022, an inquest ruled Molly had “died from an act of self-harm while suffering from depression and the negative effects of online content”. The government is preparing to launch a consultation on children’s use of smartphones and social media next month. It is expected to include proposals to ban the sale of smartphones to under-16s, make it easier for parents to put parental controls on devices, and raise the minimum age for social media apps from 13 to as high as 16. Russell said “overly intrusive” parental controls could weaken trust between children and parents and make it less likely that young people would flag harmful content and interactions. He added that banning social media for under-16s would be punishing them and not social media companies. “This would punish children for the failures of technology companies to build their products responsibly,” he said, adding that “children have told us that being online is fundamental to their lives”. Russell described a mooted smartphone sale ban as “naive”, because such a move would at best delay encounters with harmful online content and would not remove the danger. The Russell family has set up the Molly Rose Foundation – chaired by Ian Russell – to campaign for improved online safety. Russell said, notwithstanding his concerns over the proposals, it was clear that further action needed to be taken despite the recent introduction of the Online Safety Act, which imposes a duty of care on social media companies to shield children from harmful content. “Children and young people continue to face a wave of inherently preventable online harms on often negligent-by-design social media platforms,” he said. Last year, Russell said the response of tech companies to his daughter’s inquest, where the coroner recommended improving child safety on social media sites, was “underwhelming and unsurprising”. Russell called for a “strengthened” Online Safety Act, which he wants to see bolstered with tougher regulations on self-harm content and enhanced scrutiny of platforms’ inner workings. He said this could be achieved by the next government introducing “strong yet measured” follow-up legislation in the next parliament. A government spokesperson said: “Our commitment to making the UK the safest place to be a child online is unwavering, as evidenced by our landmark online safety act.”",
        "author": "Dan Milmo",
        "published_date": "2024-04-30T07:00:38+00:00"
    },
    {
        "id": "68efdd40-69b3-41a8-96d6-b3a0336536b7",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/29/eu-to-investigate-meta-over-election-misinformation-ahead-of-june-polls",
        "title": "EU to investigate Meta over election misinformation before June polls",
        "content": "The EU is set to launch formal proceedings against Meta, the owner of Facebook and Instagram, amid concerns it is not doing enough to counter Russian disinformation before the EU elections in June, according to reports. It is also expected to express concerns about the lack of effective monitoring of election content and a potentially inadequate mechanism for flagging illegal content. It is understood the European Commission is concerned that Meta’s moderation system is not robust enough to counterbalance the potential proliferation of fake news and attempts to suppress voting. The Financial Times reported that officials were particularly worried about the way Meta’s platforms were handling Russia’s efforts to undermine upcoming European elections, although it was expected to stop short of citing the Kremlin in proceedings. Reports suggest that the commission is particularly concerned over Meta’s plan to discontinue CrowdTangle, a public insights tool that allows real-time disinformation researchers, journalists and others across the EU to monitor the spread of fake news and attempts to suppress voting. Under sweeping new laws forcing tech companies to regulate their own content for compliance with the law in the EU, Facebook and others are obliged to have systems to guard against the systemic risk of election interference. A spokesperson for Meta said: “We have a well-established process for identifying and mitigating risks on our platforms. We look forward to continuing our cooperation with the European Commission and providing them with further details of this work.” If the move on Meta is confirmed it will come just days after the commission carried out stress tests on all the big social media platforms to determine whether there were adequate safeguards in place against Russian disinformation. The stress tests entailed a series of fictitious scenarios based on historical attempts at influencing elections as well as cyber-enabled information manipulation. This included deepfakes and attempts to suppress authentic opinions through online harassment and threats. Such opinion suppression was identified by the EU in February as a new weapon in silencing legitimate democratic voices. “The aim was to test platforms’ readiness to address manipulative behaviour that could occur in the run-up to the elections, in particular the different manipulative tactics, techniques and procedures,” said the commission. This allowed them to test the resilience of social media to manipulation, which politicians predict will intensify in the next six weeks. The European parliamentary elections are being held on 6-9 June against a backdrop of increasing disinformation across the bloc. On Monday the parliament released tips for voters with a list of previous incidents, including claims that only pens with certain coloured ink will be accepted on ballot papers. Politicians have also warned voters to be on the lookout for disinformation, given the experience of recent national elections. In elections in Slovakia, Spain, Finland and Estonia, stories that voting booths had pens with disappearing ink spread on social media, while voters were told of physical threats including bombs at polling stations during last year’s Spanish election. The EU DisinfoLab has tracked 17,000 incidents of disinformation of fake news with many attempts to discredit Ukraine’s defence in the war against Russia, including Vladimir Putin’s pseudo-historical grounds for his invasion. Last week a Czech news agency website was hacked to display fake news. One of the articles claimed that the Czech counterintelligence service had prevented an assassination attempt on the Slovak president, Peter Pellegrini; another carried an alleged reaction from the Czech foreign minister, Jan Lipavský, to the news. Last month the Czech government uncovered what it believed was a Moscow-orchestrated disinformation network. The Belgian prime minister also recently revealed the federal prosecutor had opened an investigation into alleged payments of MEPs by Russia with a view to electing more pro-Russian deputies to the European parliament.",
        "author": "Lisa O'Carroll",
        "published_date": "2024-04-29T15:57:39+00:00"
    },
    {
        "id": "a3636c2d-b9e5-4f91-9558-dd78fc60d6da",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/29/devices-with-weak-passwords-to-be-banned-uk",
        "title": "No more 12345: devices with weak passwords to be banned in UK",
        "content": "Tech that comes with weak passwords such as “admin” or “12345” will be banned in the UK under new laws dictating that all smart devices must meet minimum security standards. Measures to protect consumers from hacking and cyber-attacks come into effect on Monday, the Department for Science, Innovation and Technology said. It means manufacturers of phones, TVs and smart doorbells, among others, are now legally required to protect internet-connected devices against access by cybercriminals, with users prompted to change any common passwords. Brands have to publish contact details so that bugs and issues can be reported, and must be transparent about timings of security updates. It is hoped the new measures will help give customers confidence in buying and using products at a time when consumers and businesses have come under attack from hackers at a soaring rate. The consumer champion Which? said it had been instrumental in pushing for the new measures and it welcomed the changes. Rocio Concha, a director of policy and advocacy at Which?, said: “The OPSS [Office for Product Safety and Standards] must provide industry with clear guidance and be prepared to take strong enforcement action against manufacturers if they flout the law, but we also expect smart device brands to do right by their customers from day one and ensure shoppers can easily find information on how long their devices will be supported and make informed purchases.” The science and technology minister, Jonathan Berry, said: “As everyday life becomes increasingly dependent on connected devices, the threats generated by the internet multiply and become even greater. “From today, consumers will have greater peace of mind that their smart devices are protected from cybercriminals, as we introduce world-first laws that will make sure their personal privacy, data and finances are safe. We are committed to making the UK the safest place in the world to be online and these new regulations mark a significant leap towards a more secure digital world.” The laws are taking effect as part of the product security and telecommunications infrastructure (PSTI) regime, which aims to strengthen the UK’s resilience against cybercrime.",
        "author": "",
        "published_date": "2024-04-28T23:01:17+00:00"
    },
    {
        "id": "077b9764-c1c0-43bf-b25f-7700f744bbaa",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/28/the-science-isnt-there-do-dating-apps-really-help-us-find-our-soulmate",
        "title": "‘The science isn’t there’: do dating apps really help us find our soulmate?",
        "content": "A class-action lawsuit filed in a US federal court last Valentine’s Day accuses Match Group – the owners of Tinder, Hinge and OkCupid dating apps, among others – of using a “predatory business model” and of doing everything in its power to keep users hooked, in flagrant opposition to Hinge’s claim that it is “designed to be deleted”. The lawsuit crystallised an ocean of dissatisfaction with the apps, and stimulated a new round of debate over their potential to harm mental health, but for scientists who study romantic relationships it sidestepped the central issue: do they work? Does using the apps increase your chances of finding your soulmate, or not? The answer is, nobody knows. “The science isn’t there,” says sociologist Elizabeth Bruch of the University of Michigan, who has studied online dating for a decade. The apps have undoubtedly “altered social reality”, to quote the lawsuit. In the US, where uptake has been greatest since their advent, first as websites, about 30 years ago, more than half of all heterosexual couples – and an even higher proportion of gay couples – now meet online, according to Stanford University sociologist Michael Rosenfeld. Europe, slower to catch on, still has an estimated 80 million users. Rosenfeld, who tracks US dating trends, says that online dating has steadily replaced traditional ways of finding mates, through friends, work or places of worship. It might eventually plateau at a certain market share, since those other channels haven’t gone away, but reports that gen Z – which includes those now in their 20s – are turning their back on the apps are not borne out by his data. The recent and real dating drought is more likely to be a temporary blip caused by the pandemic, Rosenfeld says, which made it nearly impossible for people to follow up online connections by meeting face-to-face. “The number of single adults in the United States increased by about 10 to 12 million during the pandemic,” he says. But even if online dating is now a permanent fixture of our social landscape, research is lacking on how happy or durable the couples are that meet that way, or on whether the apps are presenting users with the most suitable candidates. The companies don’t generally release their data, but Bruch says there’s no evidence that they have any greater inside knowledge than the scientists who study the question. One recent US study showed that people who met their spouse online reported having slightly less satisfying and stable marriages than those who met them offline, but this could be explained by factors other than the online dating experience itself, such as the lingering stigma associated with meeting people that way, and the typically greater geographical distance between the two halves of the couple. “Nobody really knows what makes for a great relationship and what makes for chemistry and what makes for long-term compatibility,” says Bruch. Meanwhile, dating platforms promising new and better ways to unite kindred spirits have been proliferating. Some claim to be able to match couples on the basis of their brain activity or facial expressions. In the relative data desert, who’s to say they can or can’t? What we do know, from online forums where people discuss their experiences, is that for every couple united online there is plenty of disappointment. One woman whose story haunts Bruch said she swiped through more than 40,000 profiles and did not end up in a relationship. “Allowing three minutes to look at a profile, that’s 2,000 hours or 12 weeks of her life looking for partners,” Bruch says. Of course, offline methods can be frustrating too, but what if they could all be improved? The technology that enables online dating presents a golden opportunity for collecting the data that, until now, has been so hard to come by, and for developing the missing science of human connection. And since the companies are so secretive, and commercially oriented, a number of academic research groups have begun building their own apps – ones that will double as matchmaker and research tool. Bruch and University of Michigan psychologist Amie Gordon will roll out their free app this summer, to the local student population to begin with, and they hope to have preliminary findings by December. Bruch says that unlike the commercial alternatives, theirs will be launched with full disclosure: “We don’t know who you’re compatible with.” Their collaboration grew out of conversations that made it clear to them that psychologists and sociologists were addressing different parts of the problem. Gordon, who is interested in what keeps couples together, pointed to psychological research showing no correlation between a couple being well-matched in age, ethnic identity or level of education, and long-term compatibility. Bruch laughed when she heard that. “That’s because people have already selected for those things by the time they get together,” she says. Sociologists had shown that similarity on those measures counts in the early stages of a relationship. Compared with the population as a whole, therefore, couples do score highly for similarity. Overall, though, it’s not clear when similarity counts, or in what domains. Similar values seem to matter more than similar personalities, especially at the stage of initial attraction, but Gordon says that’s one of the few messages that can be extracted from the messy data. The pair realised that in order to get a clearer picture of the dynamics of romance, they needed to follow couples through time, starting from the pool of uncoupled, potential partners. That meant resolving some tricky ethical issues, such as how to present people with candidates they might not have chosen themselves. * * * In 2014, one of OkCupid’s founders caused uproar when he admitted that the company had experimented by presenting users with badly matched profiles, telling them they were good matches, to see if the pairs would converse as much as genuinely well-matched ones (they did). The experiment was widely judged to be unethical, but it did suggest that who people think they are compatible with isn’t necessarily who they are compatible with. Ethicist Luke Brunning of Leeds University, who with fellow ethicist Natasha McKeever is also building a research-oriented dating app, says that one of the problems with the commercial products is that they allow users to filter their searches according to their own preconceptions of compatibility, which might not be justified. “Relating to other people is a complicated thing that takes work and effort,” he says. “It unfolds in unpredictable ways that often surprise us when we look back on it.” Bruch and Gordon are getting around this with a trade-off: in return for seeing the profiles they choose most of the time, users must agree to participate in “Serendipity Sundays” where they have less control over who is presented to them. “The goal is to be as transparent as we can,” says Gordon. “We’re telling people: experiment with us.” To be fair to the companies, Brunning says, they have been innovating themselves – to protect users from cyberstalking, for example. But he and McKeever are interested in exploring less studied aspects of the user experience, such as the sense of alienation that some users complain about, and discrimination against minority groups. One key question will be what exactly people are using the apps for, since it’s clear their goal isn’t always to secure a date. Some might be passing time with a flirtation, for example, while others might be looking to make connections in a new city. Ultimately the researchers hope the companies will use their findings to hone apps that work better for all users. Sceptics may feel that’s unlikely, since the companies’ drive to maximise profits is incompatible with many users’ wish to find love and delete the app. The ongoing US lawsuit accuses Match Group of deploying “addictive, game-like design features, which lock users into a perpetual pay-to-play loop”. There is some evidence that people can become addicted to dating apps, and although more research is needed, anthropologist Natasha Schüll of New York University thinks there’s merit in the lawsuit’s claim. She spent 15 years observing slot machine players in Las Vegas, reporting her findings in the highly acclaimed book Addiction By Design (2012), and she sees clear parallels between dating apps and slot machines. They hook people with the promise of love rather than riches, she says, but they hold them in the same way – through the game-like design of their interfaces, which engage the brain’s reward circuits. Some habitual players of slot machines report entering a trance-like state as they repetitively spin the wheels. “Gamblers talk about this as the thing that they become addicted to – to the point where winning a jackpot irritates them because it stops the flow of the game,” says Schüll. “I have heard people say this about dating apps.” But dating apps, she says, are far from alone in exploiting the hook-and-hold mechanism of slot machines. “To my mind, the bigger criticism here is of the contemporary model of capitalism – the click economy,” she says. Bruch worries that the convergence of two trends – gamification, and the shift of dating online – means that people are being forced to run the gauntlet of addiction to satisfy a basic human need. Rosenfeld is less concerned. The dating apps may have some manipulative aspects, he says, but so does your mother, or the village matchmaker of the past. The apps have other advantages, too. Many female users appreciate being able to deal with daters remotely to begin with, he says. And as for the science of connection, your mum also used a pretty wonky algorithm to funnel suitors your way – generally speaking, selecting “people like us”. Rosenfeld’s long experience of interviewing singles leaves him in no doubt that it’s hard to find your soulmate, however you do it. “The frogs outnumber the princes, and if you’ve been out there and if you’ve kissed some frogs, you’re bound to be frustrated,” he says. Ultimately, suggests Rosenfeld, dating apps generate several million relationships a year in the US alone, leading him to rate them a net social positive. They should be regulated to rein in the gamified elements and increase transparency, he says, but that’s true of all social media, and it’s more urgent for platforms such as Facebook that have huge followings and are more vulnerable to external manipulation. Better to work to improve the apps, and to learn from them, than to vilify them, he says, because one thing is certain: “They are here to stay.”",
        "author": "Laura Spinney",
        "published_date": "2024-04-28T14:00:06+00:00"
    },
    {
        "id": "7c97dac7-b4da-4815-8e55-c236b1308e3d",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/28/bbc-presenters-likeness-used-in-advert-after-firm-tricked-by-ai-generated-voice",
        "title": "BBC presenter’s likeness used in advert after firm tricked by AI-generated voice",
        "content": "There was something strange about her voice, they thought. It was familiar but, after a while, it started to go all over the place. Science presenter Liz Bonnin’s accent, as regular BBC viewers know, is Irish. But this voice message, ostensibly granting permission to use her likeness in an ad campaign, seemed to place her on the other side of the world. The message, it turns out, was a fake – AI-generated to mimic Bonnin’s voice. Her management team got hold of it after they saw the presenter’s face on online ads for an insect repellant spray this week, something for which she did not sign up. “At the very beginning it does sound like me but then I sound a bit Australian and then it’s definitely an English woman by the end. It’s all fragmented and there’s no cadence to it,” said Bonnin, best known for presenting Bang Goes the Theory and Our Changing Planet. “It does feel like a violation and it’s not a pleasant thing,” she added. “Thank goodness it was just an insect repellant spray and that I wasn’t supposedly advertising something really horrid!” Howard Carter, the chief executive of Incognito, the company behind the botched campaign, claims he was sent a number of voice messages by someone he thought was Bonnin. He said these voice messages “clinched it” for him that he was really speaking to her. He had previously sought her endorsement before being approached by a Facebook profile adopting Bonnin’s identity. He claims the messages exchanged between the two led him to believe she was the real deal despite thinking the profile was “a bit suspect”. The person assuming Bonnin’s identity gave Carter a phone number and email address. They also provided him with contact details from someone pretending to be from the Wildlife Trusts, the charity where Bonnin serves as president. He said the deal was negotiated via WhatsApp and emails. He also claims he spoke to one of the scammers impersonating Bonnin over the phone on at least one occasion. On 13 March, Carter received a contract via email that he believed was electronically signed by Bonnin. The company sent £20,000 to an account linked to a digital bank on 15 March, bank statements show. Images of Bonnin for use in the campaign were sent five days later. Further emails sent by Incognito went unanswered. The campaign launched on Monday, using quotes and images sent by the scammers. Hours later, Bonnin said on X that she did not agree to any endorsement with the company. The person impersonating Bonnin texted Carter on Tuesday to apologise and said she was not handling her social media and also cited server and security issues. At this point, the penny dropped for Carter that he had been duped. Bonnin said: “I’m very sorry for what the company has gone through. It’s not fun for them at all, but it’s a violation on both our parts. It is a reminder that, if it looks too good to be true and too easy, or a little bit strange, triple check or quadruple check.” She added: “If somebody contacts you and says, ‘Hey, let’s not go through the professional route,’ then beware.” Carter claims he did not get the deal signed off through Bonnin’s management agency because the person impersonating her said “she was doing us a favour, provided we do it direct with her and not involve her main agency”. In a voice note heard by the Guardian, the AI-generated Bonnin says a number of stilted phrases, such as “I must thank you for providing clarity on the direction you envision for our collaboration”. Two AI experts who assessed the voice note agreed it was likely to have been artificially generated. Surya Koppisetti, a senior applied scientist at the image detection startup Reality Defender said: “There are gaps and recitation speed issues that are consistent with AI-generated speech. The dialogue is inconsistent in accent.” Koppisetti added that the speech quality was “unusually clear despite the noise”. Michael Keeling, a senior data scientist at the AI technology company Faculty, said the “steady, monotone” artificial background noise in the message “is a classic way of making something seem more realistic”. “If you’re listening to this from your phone on a busy street, it’s much easier to slip through that way,” he added. Bonnin said the incident was a “warning message” about the potential pitfalls of AI. “There are many ways AI can be used to benefit society. We’ve heard it can be used to identify cancers, but it is also deeply unconcernedly not regulated enough. The technology is only going to get better and more sophisticated,” she added. Incognito said it had reported the incident to the police and to its bank. In a statement, it said: “We hope [our CEO] falling for this elaborate scam will be a warning to others not to fall for the same or similar ruses. Sophisticated criminals using AI and other computer-generated communications have targeted Howard and Incognito a few times. “Understandably, many companies do not want the public to hear they’ve been deceived because of the embarrassment and shame. As an ethical, transparently run company, we feel it is our duty to alert people and their businesses about the rise of clever deceptions like this. Howard and everybody else in our organisation apologises to Liz Bonnin and her associates for any harm we have inadvertently caused.” The BBC did not respond to a request for comment. The Wildlife Trusts declined to comment.",
        "author": "Sammy Gecsoyler",
        "published_date": "2024-04-28T09:39:54+00:00"
    },
    {
        "id": "1a7fe674-d61a-4453-8147-6eecc6916017",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/28/nick-bostrom-controversial-future-of-humanity-institute-closure-longtermism-affective-altruism",
        "title": "‘Eugenics on steroids’: the toxic and contested legacy of Oxford’s Future of Humanity Institute",
        "content": "Two weeks ago it was quietly announced that the Future of Humanity Institute, the renowned multidisciplinary research centre in Oxford, no longer had a future. It shut down without warning on 16 April. Initially there was just a brief statement on its website stating it had closed and that its research may continue elsewhere within and outside the university. The institute, which was dedicated to studying existential risks to humanity, was founded in 2005 by the Swedish-born philosopher Nick Bostrom and quickly made a name for itself beyond academic circles – particularly in Silicon Valley, where a number of tech billionaires sang its praises and provided financial support. Bostrom is perhaps best known for his bestselling 2014 book Superintelligence, which warned of the existential dangers of artificial intelligence, but he also gained widespread recognition for his 2003 academic paper “Are You Living in a Computer Simulation?”. The paper argued that over time humans were likely to develop the ability to make simulations that were indistinguishable from reality, and if this was the case, it was possible that it had already happened and that we are the simulations. I interviewed Bostrom more than a decade ago, and he possessed one of those elusive, rather abstract personalities that perhaps lend credence to the simulation theory. With his pale complexion and reputation for working through the night, he looked like the kind of guy who didn’t get out much. The institute seems to have recognised this social shortcoming in its final report, a long epitaph written by FHI research fellow Anders Sandberg, which stated: “We did not invest enough in university politics and sociality to form a long-term stable relationship with our faculty… When epistemic and communicative practices diverge too much, misunderstandings proliferate.” Like Sandberg, Bostrom has advocated transhumanism, the belief in using advanced technologies to enhance longevity and cognition, and is said to have signed up for cryogenic preservation. Although proudly provocative on the page, he was wary and defensive in person, as if he were privy to an earth-shattering truth that required vigilant protection. His office, located in a medieval backstreet, was a typically cramped Oxford affair, and it would have been easy to dismiss the institute as a whimsical undertaking, an eccentric, if laudable, field of study for those, like Bostrom, with a penchant for science fiction. But even a decade ago, when I paid my visit, the FHI was already on its way to becoming the billionaire tech bros’ favourite research group. In 2018 it received £13.3m from the Open Philanthropy Project, a non-profit organisation backed by Facebook co-founder Dustin Moskovitz. And Elon Musk has also been a benefactor. Bostrom’s warnings on AI were taken seriously by big tech. But as competition has heated up in the race to create a general artificial intelligence, ethics have tended to take a back-seat. Among the other ideas and movements that have emerged from the FHI are longtermism – the notion that humanity should prioritise the needs of the distant future because it theoretically contains hugely more lives than the present – and effective altruism (EA), a utilitarian approach to maximising global good. These philosophies, which have intermarried, inspired something of a cult-like following, which may have alienated many in the wider philosophy community in Oxford, and indeed among the university’s administrators. According to the FHI itself, its closure was a result of growing administrative tensions with Oxford’s faculty of philosophy. “Starting in 2020, the Faculty imposed a freeze on fundraising and hiring. In late 2023, the Faculty of Philosophy decided that the contracts of the remaining FHI staff would not be renewed,” the final report stated. But both Bostrom and the institute, which brought together philosophers, computer scientists, mathematicians and economists, have been subject to a number of controversies in recent years. Fifteen months ago Bostrom was forced to issue an apology for comments he’d made in a group email back in 1996, when he was a 23-year-old postgraduate student at the London School of Economics. In the retrieved message Bostrom used the N-word and argued that white people were more intelligent than black people. The apology did little to placate Bostrom’s critics, not least because he conspicuously failed to withdraw his central contention regarding race and intelligence, and seemed to make a partial defence of eugenics. Although, after an investigation, Oxford University did accept that Bostrom was not a racist, the whole episode left a stain on the institute’s reputation at a time when issues of anti-racism and decolonisation have become critically important to many university departments. It was Émile Torres, a former adherent of longtermism who has become its most outspoken critic, who unearthed the 1996 email. Torres says that it’s their understanding that it “was the last straw for the Oxford philosophy department”. Torres has come to believe that the work of the FHI and its offshoots amounts to what they call a “noxious ideology” and “eugenics on steroids”. They refuse to see Bostrom’s 1996 comments as poorly worded juvenilia, but indicative of a brutal utilitarian view of humanity. Torres notes that six years after the email thread, Bostrom wrote a paper on existential risk that helped launch the longtermist movement, in which he discusses “dysgenic pressures” – dysgenic is the opposite of eugenic. Bostrom wrote: “Currently it seems that there is a negative correlation in some places between intellectual achievement and fertility. If such selection were to operate over a long period of time, we might evolve into a less brainy but more fertile species, homo philoprogenitus (‘lover of many offspring’).” Bostrom now says that he doesn’t have any particular interest in the race question, and he’s happy to leave it to others with “more relevant knowledge”. But the 28-year-old email is not the only issue that Oxford has had to consider. As Torres says, the effective altruism/longtermist movement has “suffered a number of scandals since late 2022”. Just a month before Bostrom’s incendiary comments came to light, the cryptocurrency entrepreneur Sam Bankman-Fried was extradited from the Bahamas to face charges in the US relating to a multibillion-dollar fraud. Bankman-Fried was a vocal and financial supporter of effective altruism and a close friend of William MacAskill, an academic who has strong links to the FHI and who set up the Centre for Effective Altruism, where Bankman-Fried worked briefly. It was MacAskill who was said to have persuaded Bankman-Fried a decade ago to seek to earn as much money as possible so that he could give it away. The entrepreneur seemed to follow the first part of that injunction, but then went on to spend $300m in fraudulently earned money on Bahamian real estate. His downfall and subsequent 25-year prison sentence have done little for the moral arguments put forward by the FHI and its associate groups. If that wasn’t enough, the coup last November that briefly dislodged Sam Altman as the CEO of Open AI, the company behind ChatGPT, was attributed to company board members who were supporters of EA. Altman’s speedy return was seen as a defeat for the EA community, and, says Torres, “has seriously undermined the influence of EA/longtermism within Silicon Valley”. All of this, of course, seems a long way from the not insubstantial matter of preserving humanity, which is the cause for which the FHI was ostensibly set up. No doubt that noble endeavour will find other academic avenues to explore, but perhaps without the cultish ideological framework that left the institute with a bright future behind it.",
        "author": "Andrew Anthony",
        "published_date": "2024-04-28T05:00:02+00:00"
    },
    {
        "id": "ee1fe4ad-fa27-4330-baf1-c48c9491cf19",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/27/google-project-nimbus-israel",
        "title": "Workers accuse Google of ‘tantrum’ after 50 fired over Israel contract protest",
        "content": "Google has been accused of throwing a “tantrum” after sacking more than 50 workers in response to a protest over the company’s military ties to the Israeli government – firings that have shone a light on a controversial project and long-simmering tensions between staff and management. The workers were sacked following protests at Google offices in New York City and Sunnyvale, California, organized by No Tech for Apartheid – an alliance of Google and Amazon workers who have been protesting against a $1.2bn contract with the Israeli government called Project Nimbus that they claim will make it “easier for the Israeli government to surveil Palestinians and force them off their land”. Initially, Google fired 28 workers over the protests and then fired more than 20 workers a few days later. The firings are the largest to occur since Israel’s military campaign in response to the 7 October 2023 terrorist attack by Hamas in which about 1,200 people were killed and more than 200 hostages were taken. Since then, more than 34,000 Palestinians have been killed in Gaza, including more than 14,000 children and 9,670 women. Google has fired and reprimanded workers for participating in protests before, such as over a 2018 walkout and sit-in protest about sexual harassment issues at the company, but not previously to this extent. In March, Google fired a cloud engineer who protested against an Israel tech event in New York City. Emaan Haseem, a software engineer at Google and organizer with No Tech for Apartheid, was one of the fired workers. “Many of us had just recently gotten promoted. I was the fastest promoted person underneath my manager,” she said. This was a peaceful protest, she said, “with high visibility, high transparency, that we livestreamed. Everything and everybody could see how it went.” Haseem said the sit-in protests were a response to Google’s refusal to engage with workers’ concerns. “Look at the way Google has overreacted, so emotionally, and has lashed out at 50 workers over this contract rather than giving any more transparency, clarity, or attempting to prove they are not especially providing the Israeli military resources to aid and abet their genocide and continue their apartheid,” added Haseem. “They fully had the option to do that but instead they chose to throw a tantrum and take it out on 50 workers, many of whom were not involved in the sit-in. “It was done so emotionally, so irrationally, that Google has also taken its mask off in the process. It has shown its honest and true self, how contradictory they are, how they don’t actually care about doing the right thing, about not being evil, about their values where we should speak up and speak out against anything wrong that we see happening in our work or in the workplace.” Since the contract was awarded in 2021, workers at Google and Amazon have been organizing in opposition to the corporations’ joint contract with the Israeli military and government. The $1.2bn contract to provide cloud services “allows for further surveillance of and unlawful data collection on Palestinians, and facilitates expansion of Israel’s illegal settlements on Palestinian land”, according to an op-ed workers wrote in 2021. The firings have disrupted the financial stability of workers, Haseem said, but she and others affected have received significant support from co-workers and others. She also said that one of the biggest challenges with the campaign against Project Nimbus was outreach and educating others on the issues – something which Google’s firings had only facilitated. Hasan Ibraheem, a Google software engineer in New York who was fired and arrested for participating in the protest, said he and other workers were put on administrative leave, losing corporate access, and then fired the next day by email en masse. “We don’t know of anyone who had actually been reached out to by HR. We were asked no questions. There was no consulting with us. No one asked us anything. It was just a very cold mass email sent out, you are now fired, goodbye, because they don’t want to deal with us, they want to silence us and we’re not going to stay silent,” said Ibraheem. “We don’t want our labor to be used for aiding a genocide and that’s why we did that action and we are going to continue fighting to have this project dropped.” The workers declined to comment on any potential legal proceedings they may pursue in response to the firings. No Tech for Apartheid called the firings “illegal” in a blogpost in response to Google’s actions. A spokesperson for Google said in an email on the firings: “We continued our investigation into the physical disruption inside our buildings on April 16, looking at additional details provided by coworkers who were physically disrupted, as well as those employees who took longer to identify because their identity was partly concealed – like by wearing a mask without their badge – while engaged in the disruption. Our investigation into these events is now concluded, and we have terminated the employment of additional employees who were found to have been directly involved in disruptive activity.” They denied firing any employees not involved. Google also denied protesters’ characterizations of Project Nimbus, stating: “We have been very clear that the Nimbus contract is for workloads running on our commercial cloud by Israeli government ministries, who agree to comply with our Terms of Service and Acceptable Use Policy. This work is not directed at highly sensitive, classified, or military workloads relevant to weapons or intelligence services.”",
        "author": "Michael Sainato",
        "published_date": "2024-04-27T11:00:34+00:00"
    },
    {
        "id": "ad5958b0-da69-4035-adfc-9571869f2424",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/26/tesla-autopilot-fatal-crash",
        "title": "Tesla Autopilot feature was involved in 13 fatal crashes, US regulator says ",
        "content": "US auto-safety regulators said on Friday that their investigation into Tesla’s Autopilot had identified at least 13 fatal crashes in which the feature had been involved. The investigation also found the electric carmaker’s claims did not match up with reality. The National Highway Traffic Safety Administration (NHTSA) disclosed on Friday that during its three-year Autopilot safety investigation, which it launched in August 2021, it identified at least 13 Tesla crashes involving one or more death, and many more involving serious injuries, in which “foreseeable driver misuse of the system played an apparent role”. It also found evidence that “Tesla’s weak driver engagement system was not appropriate for Autopilot’s permissive operating capabilities”, which resulted in a “critical safety gap”. The NHTSA also raised concerns that Tesla’s Autopilot name “may lead drivers to believe that the automation has greater capabilities than it does and invite drivers to overly trust the automation”. Tesla said in December that its largest-ever recall, covering 2.03m US vehicles – or nearly all of its vehicles on US roads – was to better ensure drivers pay attention when using its advanced driver-assistance system. After closing the first investigation, regulators opened another, this one into whether that recall to install new Autopilot safeguards was adequate. The NHTSA said it was opening the second investigation after identifying concerns due to crash events after vehicles had had the recall software update installed “and results from preliminary NHTSA tests of remedied vehicles”. That recall investigation covers models Y, X, S, 3 and Cybertruck vehicles in the US equipped with Autopilot and produced in the 2012 to 2024 model years, NHTSA said. The agency said Tesla has issued software updates to address issues that appear related to its concerns but has not made them “a part of the recall or otherwise determined to remedy a defect that poses an unreasonable safety risk”. The NHTSA also cited Tesla’s statement “that a portion of the remedy both requires the owner to opt in and allows a driver to readily reverse it”. Tesla said in December that Autopilot’s software system controls “may not be sufficient to prevent driver misuse” and could increase the risk of a crash. Tesla did not immediately respond to a request for comment. In February, Consumer Reports, a non-profit organization that evaluates products and services, said its testing of Tesla’s Autopilot recall update found that changes did not adequately address many safety concerns raised by the NHTSA and urged the agency to require the automaker to take “stronger steps”, saying Tesla’s recall “addresses minor inconveniences rather than fixing the real problems”. Tesla’s Autopilot is intended to enable cars to steer, accelerate and brake automatically within their lane, while enhanced Autopilot can assist in changing lanes on highways but does not make vehicles autonomous. One component of Autopilot is Autosteer, which maintains a set speed or following distance and works to keep a vehicle in its driving lane. Tesla said in December it did not agree with the NHTSA’s analysis but would deploy an over-the-air software update that will “incorporate additional controls and alerts to those already existing on affected vehicles to further encourage the driver to adhere to their continuous driving responsibility whenever Autosteer is engaged”. The NHTSA’s then top official, Ann Carlson, said in December that the investigation determined that more needed to be done to ensure drivers are engaged when Autopilot is in use. “One of the things we determined is that drivers are not always paying attention when that system is on,” Carlson said. The NHTSA opened its August 2021 investigation of Autopilot after identifying more than a dozen crashes in which Tesla vehicles hit stationary emergency vehicles. Separately, since 2016, the NHTSA has opened more than 40 special Tesla crash investigations in cases where driver systems such as Autopilot were suspected of being used, with 23 crash deaths reported to date. Tesla’s recall includes increasing prominence of visual alerts and the disengaging of Autosteer if drivers do not respond to inattentiveness warnings and additional checks upon engaging Autosteer. Tesla said it would restrict Autopilot use for one week if significant improper usage is detected. Tesla disclosed in October that the US justice department issued subpoenas related to its Full Self-Driving (FSD) and Autopilot features. Reuters reported in October 2022 that Tesla was under criminal investigation. Tesla in February 2023 recalled 362,000 US vehicles to update its FSD beta software after the NHTSA said the vehicles did not adequately adhere to traffic safety laws and could cause crashes.",
        "author": "",
        "published_date": "2024-04-26T19:20:44+00:00"
    },
    {
        "id": "412c9502-37f8-40e2-b0eb-6de0cd689963",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/26/robot-flamethrower-dog-thermonator",
        "title": "Ohio company to sell a ‘flamethrower-wielding robot dog’ called the Thermonator",
        "content": "What has four legs and can breathe fire? Apparently the Thermonator, a controversial “first-ever flamethrower-wielding robot dog”. According to Throwflame, an Ohio-based company that manufactures flamethrowers, its latest invention features a 30ft firing range, light detection and range mapping, as well as laser sighting, aboard a battery-powered thing with legs that can jump around. Touted as “your ultimate firepower companion”, the robot’s uses include wildlife control and prevention, agricultural management, ecological conservation, and snow and ice removal, as well as entertainment and special effects, Throwflame said. For just $9,420 and in the absence of a pet dragon, you can have your own fire-breathing best friend of canine appearance. However, not everyone is onboard. Following reports of the Thermonator’s release, some people were swift to mock the innovation on social media with comments such as “Just because we can, doesn’t mean we should.” Another person said: “Guess I’m the only one who watched that Black Mirror episode,” in apparent reference to the 2017 episode Metalhead from the television series Black Mirror which explores the consequences of new technologies. In the episode, a woman attempts to survive a post-apocalyptic world overrun with deadly robot dogs. Another user wrote: “Cool. Will the owners be held liable when this inevitably starts wildfires? Or sets their neighbors’ house on fire?” Currently, flamethrowers are legal across the US, except in Maryland, where they are considered a “destructive device”. Under Maryland’s state law, destructive devices mean explosive, incendiary or toxic material that is combined with a delivery or detonating apparatus so as to be capable of inflicting injury to persons or damage to property. They also include devices that have been deliberately modified, containerized or otherwise equipped with a special delivery, activation or detonation component that gives the material destructive characteristics of a military ordnance. Meanwhile in California, residents are required to obtain a permit if they wish to possess a flamethrower that has a firing range of more than 10ft, CNN reported.",
        "author": "Maya Yang",
        "published_date": "2024-04-26T17:20:26+00:00"
    },
    {
        "id": "32ad9e36-75f7-4cd3-a0ac-f0276054928c",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/26/google-alphabet-2tn-valuation-dividend-shares-results",
        "title": "Google parent Alphabet hits $2tn valuation as it announces first dividend",
        "content": "Google’s parent company has hit a stock market value of $2tn (£1.6tn) as investors reacted to a declaration of its first ever dividend alongside strong results on Thursday. Shares in Alphabet rose 10% in early Wall Street trading on Friday to give the tech group a stock market capitalisation – a measure of a corporation’s value – of more than $2tn. Alphabet last hit that level in intraday trading in 2021, but Friday was the first time it has closed above that benchmark. Alphabet’s shares rose after it posted results on Thursday that exceeded analysts’ expectations. Microsoft also reported strong figures on Thursday, amid heavy investment in artificial intelligence, and investors pushed the company past the $3tn mark at market close on Friday, a level it has already crossed this year. Alphabet’s quarterly figures included better than expected results from its core Google search business as well as its YouTube platform, and strong figures from its cloud business, which has been boosted by the training and operation of artificial intelligence models. The company also announced its first ever dividend. Russ Mould, the investment director at AJ Bell, an investment platform, said Alphabet joining the ranks of dividend-paying tech company was a “sign of the times”. “Big tech firms have enjoyed stellar growth over the past decade and while most remain highly innovative, their cashflows have become so strong that there’s oodles of money left over post-reinvestment in the business to reward shareholders,” he added. Alphabet joins a trio of US-listed companies with valuations of more than $2tn: Microsoft at more than $3tn; Apple at $2.6tn; and Nvidia, the leading chip supplier for AI products, at just more than $2tn. Apple also passed the $3tn mark last year.",
        "author": "Dan Milmo",
        "published_date": "2024-04-26T15:27:46+00:00"
    },
    {
        "id": "3b530a48-2fbb-420b-a947-7a77eeb04221",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/26/gmb-launches-legal-action-against-out-of-control-amazon-at-coventry-warehouse",
        "title": "GMB launches legal action against ‘out of control’ Amazon at Coventry warehouse",
        "content": "The GMB is taking legal action against Amazon in the long-running battle for recognition at its Coventry warehouse, accusing the US firm of trying to “induce” staff to leave the union. An employment tribunal claim is being lodged on behalf of five workers at the large site, known as BHX4. The GMB cites practices including anti-union messages appearing on message boards, and workers being called into meetings lasting an hour or more, at which they claim managers are critical of unions. They say QR codes have also been displayed around the building, which, when scanned by a staff member, automatically generate an email to the GMB’s membership department, cancelling their membership. Amanda Gearing, a GMB senior organiser, said: “This is a company out of control. Amazon is a multibillion-pound corporation, doing everything in its power to stop minimum wage workers from forming a union.” Under the Trade Union and Labour Relations (Consolidation) Act 1992, it is unlawful for an employer to make an offer to a worker, “for the sole or main purpose of inducing the worker not to be or seek to become a member of an independent trade union”. Lawyers working with the GMB believe the presence of the QR codes may constitute such an offer. Rosa Curling, a director of Foxglove Legal, which is involved in the case, said workers had been “hustled into anti-union propaganda seminars, then have a QR code shoved in their face that terminates their membership with just one click to quit. If only it was so easy to quit Amazon Prime!” Paul Nowak, the general secretary of the TUC, said: “Good employers recognise the value of unions. But Amazon is throwing everything at trying to stop workers from organising for better pay and conditions and from having an independent voice at work. “Its union-busting tactics should have no place in Britain, and are further proof of why Labour’s new deal for working people is so badly needed.” An Amazon spokesperson said: “Our employees have the choice of whether or not to join a union. They always have. “We regularly review our pay to ensure we offer competitive wages and benefits. Our minimum starting pay has increased to £12.30 and £13 per hour depending on location, that’s a 20% increase over two years and 50% since 2018. We also work hard to provide great benefits, a positive work environment and excellent career opportunities.” The tribunal case underlines the fraught relationship between unions and management, as they prepare for a legally binding ballot on union recognition at the site. The independent watchdog the Central Arbitration Committee granted the GMB the right to call the ballot earlier this month. It is likely to take place within weeks. To secure recognition, the GMB will need to win a majority of support in the ballot. The “yes” voters must also represent at least 40% of the more than 3,000 workers on site. The GMB has been painstakingly building up its membership at the Coventry site and other nearby warehouses over many years. Workers have held a series of strikes over the past year, demanding £15 an hour and the right to negotiate with management over pay and conditions. On Black Friday in November, they were joined on the picket line by members of unions representing Amazon workers in continental Europe and the US. The multinational is well known to be sceptical about the value of working with unions, and has been accused of union-busting tactics by the US labour regulator.",
        "author": "Heather Stewart",
        "published_date": "2024-04-26T10:44:08+00:00"
    },
    {
        "id": "3e960a16-d88d-4d81-8560-bcf94733f48f",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/26/foreign-states-targeting-sensitive-research-at-uk-universities-mi5-warns",
        "title": "Foreign states targeting sensitive research at UK universities, MI5 warns",
        "content": "MI5 has warned universities that hostile foreign states are targeting sensitive research, as ministers consider measures to bolster protections. Vice-chancellors from 24 leading institutions, including Oxford, Cambridge and Imperial College London, were briefed on the threat by the domestic security service’s director general, Ken McCallum, and National Cybersecurity Centre (NCSC) chief, Felicity Oswald. In addition, the UK government is looking at increased funding to improve security at sensitive sites. Oliver Dowden, the deputy prime minister, announced plans for a consultation on a package of measures that could include looking at key university personnel being given security clearance and a strengthened process to improve the transparency of funding, particularly with foreign institutions. The measures will be focused on a small proportion of academic work, with a particular focus on research with potential dual uses in civilian and military life. McCallum told the vice-chancellors that hostile states are targeting universities to steal technology that can “deliver their authoritarian, military and commercial priorities”, the Times reported. The government ordered a review of protections for higher education in its refreshed foreign and security policy last year amid concerns that hostile states – and particularly China – were gaining undue influence over the sector. Dowden has previously warned that some universities’ reliance on overseas funding could leave them open to being “influenced, exploited, or even coerced” by a foreign power. After the security briefing, Dowden said: “For a millennium, our universities have thrived on being open – open to ideas, open to innovation, open to being independent of government. “This is not about erecting fences, this is about balancing evolving threats and protecting the integrity and security of our great institutions.” The consultation will explore proposals to protect cutting-edge technology under development in sensitive sectors that are being targeted by states intent on stealing intellectual property to enhance their own economic and military capabilities. The NCSC and the National Protective Security Authority have also launched a tool to help universities assess their research security. Michelle Donelan, the science and technology secretary, said: “I believe that universities are on the frontlines of a battle for information. “Maintaining the UK’s world-leading reputation as an academic superpower relies on having strong safeguards to protect research from those who wish to do us harm.” Tim Bradshaw, chief executive of the Russell Group of leading research universities, said: “Russell Group universities take their national security responsibilities incredibly seriously and already work closely with government and the intelligence community to help protect UK breakthroughs in fields like AI, which are important to our national interest. “But we also recognise security is a dynamic and evolving challenge which means we need the right expertise and intelligence to keep pace with this.” Universities UK chief executive Vivienne Stern said: “For several years, Universities UK has worked with government to ensure that universities are supported and equipped to recognise and mitigate risks to national security. “This is important and necessary, and we welcome the government’s approach to working hand in hand with us to get the mechanisms right.”",
        "author": "Mabel Banfield-Nwachi",
        "published_date": "2024-04-25T23:02:17+00:00"
    },
    {
        "id": "17b8394a-8cc9-4ff5-92ff-c3594ba62b57",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/25/microsoft-earnings",
        "title": "Microsoft’s heavy bet on AI pays off as it beats expectations in latest quarter",
        "content": "Profits at Microsoft beat Wall Street’s expectations as its heavy bet on artificial intelligence continued to bear fruit in the latest quarter. The technology giant has invested billions of dollars into AI in a bid to turbocharge its growth, particularly of its cloud computing services. Its cloud computing revenue surged by more than 20%. Microsoft’s AI tools “are orchestrating a new era of AI transformation, driving better business outcomes across every role and industry”, said Satya Nadella, the chief executive of Microsoft. As the group races to integrate AI across its software and services, Nadella said its Azure cloud computing business saw the pace of deals worth $100m and $10m increase by double-digit percentages. It has also started selling its Copilot AI software add-on to small businesses. Total revenue at Microsoft increased 17% to $61.86bn during the first three months of 2024, the third quarter of its financial year, surpassing analyst expectations of some $60.88bn. Earnings per share increased 20% to $2.94, ahead of the expected $2.83. Shares in the group rose 4% during after-hours trading in New York on Thursday. With a stock market value of nearly $3tn, Microsoft is the world’s largest public company. Shares in the firm have increased by more than 30% over the past year – an impressive rise, although less than the rallies of Amazon and Google, whose stocks have risen by more than 60% and 40%, respectively. With a multibillion-dollar investment in the ChatGPT developer OpenAI, Microsoft has sought to position itself at the heart of AI’s rise – and the destination for the industry’s top talent. In March, the company announced it had hired a co-founder of Google’s Deepmind AI subsidiary, Mustafa Suleyman, as well as a co-founder of $4bn startup Inflection AI and several of its employees. In November, after OpenAI’s board ousted its co-founder and chief executive, Sam Altman, he briefly said he would decamp to Seattle and join Microsoft in a major coup for Nadella. The majority of OpenAI’s employees said they would join Altman, which would effectively obliterate the company. Altman was reinstated as chief executive days later. Microsoft is now trying to monetize its dominance in this space. The company reported in the final months of 2023 that AI contributed 6% of the revenue growth within its powerhouse Azure cloud computing division; up from 3% the previous quarter, according to Yahoo Finance. The company said on Thursday that AI-based additions to LinkedIn had increased engagement on the professional social network. Nadella highlighted LinkedIn’s AI tools for writing posts and articles, which he said helped the number of LinkedIn sessions grow by 11%. Revenue from LinkedIn increased 10%, per Microsoft’s press release. It has been signing a series of high-profile AI deals. Two days before this week’s earnings report, Microsoft announced that Coca-Cola had signed a five-year, $1.1bn contract with Azure for AI and cloud computing services. Last month, it announced a partnership with AI startup Mistral to offer access to the French company’s models in Azure.",
        "author": "Blake Montgomery",
        "published_date": "2024-04-25T22:15:40+00:00"
    },
    {
        "id": "824c8fce-7b06-416b-8c32-8172df203cb5",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/25/google-revenue-quarter-one",
        "title": "Alphabet hails ‘once-in-a-generation’ AI opportunity as revenue rises",
        "content": "Shares in Alphabet, the owner of Google and YouTube, surged after it issued its first ever dividend and revealed that profits had surged in the last quarter. Sundar Pichai, CEO, hailed the transition to artificial intelligence as a “once-in-a-generation opportunity” as his company races to integrate the technology across its business. Investors cheered the firm’s earnings, and news of a $70bn stock buyback. Google posted $80.5bn in revenue for the first quarter of 2024, up 15% on the same period last year, and reported $1.89 in earnings per share, up from $1.17 – surpassing analysts’ expectations on both counts. Shares in Alphabet were up roughly 15% in after-hours trading. The company also announced its first dividend, of $0.20 per share, and said the payout would become quarterly. “Our results in the first quarter reflect strong performance from Search, YouTube and Cloud. We are well under way with our Gemini era and there’s great momentum across the company,” Pichai said in a press release. Ruth Porat, Alphabet CFO, said revenue from ads on Google Search as well as revenue from Google Cloud fuelled overall positive growth. Revenue from YouTube and Google Cloud exceeded Wall Street’s expectations, with Cloud’s operating income quadrupling to $900m. Traffic acquisition costs – one of Alphabet’s main expenses – rose as well, increasing by 10%. Like Google, Meta issued its first-ever dividend in February. “Alphabet’s better-than-expected quarter was buoyed by strong search and YouTube ad revenue, signaling that uncertainty about how to monetise conversational search and brand media’s measurement challenges aren’t yet impacting Alphabet’s bottom line,” said Nikhil Lai, a senior analyst at Forrester Research. Internal and external turmoil have rocked Google in recent weeks. The earnings report comes amid protests by employees over Google Cloud’s contract with the Israeli government that led to 48 workers being fired. An antitrust decision over its business practices and role in the digital ad market also looms large. The company botched the rollout of its new Gemini AI tools in February, leading to a steep loss in stock value, which the tech giant seems to have shaken off. Long regarded as a workplace full of jobs that paid well and were difficult to lose, Google laid off 1,000 workers in January as it continues to shift its resources to generative artificial intelligence projects. Alphabet’s stock has continued to rise during this period, however, and reached all-time highs this year. The company’s stock price has continued to rise even as it awaits judgment in a landmark antitrust trial that began last year, in which the Department of Justice accused Google of illegally monopolizing internet search and the digital ad market. During a weeks-long trial, Google defended its business practices and argued that its search platform was simply a superior product to its competitors. The verdict is expected at some point this year and it is unclear what punitive measures Google may incur if it is found guilty. Google is facing a number of other legal challenges, including a $2.3bn lawsuit from European media companies over its digital advertising practices. The European Commission levied a $2.7bn antitrust fine against Google that the company is appealing after a court upheld the penalty in January. The company’s stock dipped earlier this year due to investor fears over a haphazard and hasty rollout of its Gemini AI tools, leading to a multibillion-dollar sell off and widespread criticism. Pichai was put on the defensive in February after Gemini’s image generator created ahistorical images that included people of color as German second world war soldiers, resulting in the company suspending public access to the tool. In recent weeks, Google has also faced scrutiny and protests over its $1.2bn contract with the Israeli government to provide cloud computing services. Google fired four dozen workers for protesting the program, with demonstrators accusing Google of being complicit in Israel’s war on Gaza. Employees held a sit-in at Google’s offices in Sunnyvale and New York offices in protest of the program, resulting in arrests and the terminations. Pichai addressed the firings in an email last week, stating that Google was not the place to “fight over disruptive issues or debate politics”. He also announced that some of Google’s divisions would go through a restructuring. Google announced additional cuts to its workforce around the same time, laying off an unspecified number of people.",
        "author": "Nick Robins-Early",
        "published_date": "2024-04-25T21:57:38+00:00"
    },
    {
        "id": "d777180b-0d9a-4c31-9cb6-e6a9f5bf6c0f",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/25/meta-value-falls-190bn-as-investors-react-to-plan-to-increase-spending-on-ai",
        "title": "Meta value falls $190bn as investors react to plan to increase spending on AI",
        "content": "Shares in Meta slumped 15% when Wall Street opened on Thursday, wiping about $190bn off the value of the Facebook and Instagram parent company, as investors reacted to a pledge to ramp up spending on artificial intelligence. Mark Zuckerberg, Meta’s founder and chief executive, said on a conference call on Wednesday that spending on the technology would have to grow “meaningfully” before the company could make “much revenue” from new AI products. Shares in Meta had been boosted in 2023 by Zuckerberg’s tough action on costs in what he described as a “year of efficiency”. A relaxation of that restraint has rattled investors after Meta raised the upper bound of its capital expenditure guidance on Wednesday, from $37bn to $40bn. Last week, Meta released Llama 3, the latest version of its AI model, alongside an image generator that updates pictures in real time while users type prompts. The company’s AI-powered assistant, Meta AI, is expanding to its platforms in more than a dozen markets outside the US with the update, including Australia, Canada, Singapore, Nigeria and Pakistan. Chris Cox, Meta’s chief product officer, said the company was “still working on the right way to do this in Europe”. The share decline follows a record gain in market value by Meta in February, when the company added $196bn to its stock market capitalisation – a measure of a company’s worth – after declaring its first dividend. At the time it was the biggest one-day gain in Wall Street history. However, weeks later, Nvidia, the leading supplier of chips for training and operating AI models, smashed that record with a $277bn gain.",
        "author": "Dan Milmo",
        "published_date": "2024-04-25T14:00:34+00:00"
    },
    {
        "id": "9ffe4eaf-d920-4ab4-8e0a-aa7c7eb33786",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/13/will-us-ban-tiktok",
        "title": "Congress passed a TikTok bill. Will the US really ban the app?",
        "content": "The House of Representatives passed a bill that would require TikTok owner ByteDance to sell the social media platform or face a total ban in the United States. The Senate passed it less than a week later. Joe Biden signed it a day after the Senate voted yes. TikTok is facing its biggest existential threat yet in the US. The app was banned in Montana last year, but courts found that prohibition unconstitutional, and it never went into effect. Here’s what you need to know about the bill, how likely TikTok is to be banned, and what that means for the platform’s 170 million US users. Is the US really trying to ban TikTok, and why? The bill that passed in the House on Wednesday is the latest salvo in an ongoing political battle over the platform, which exploded in popularity after its emergence in 2017. It quickly surpassed Facebook, Instagram, Snapchat and YouTube in downloads in 2018 and reported a 45% increase in monthly active users between July 2020 and July 2022. The platform’s meteoric rise alarmed some lawmakers, who believe that TikTok’s China-based parent company could collect sensitive user data and censor content that goes against the Chinese government. TikTok has repeatedly stated it has not and would not share US user data with the Chinese government, but lawmakers’ concerns were exacerbated by news investigations that showed China-based employees at ByteDance had accessed non-public data about US TikTok users. TikTok has argued that US user data is not held in China but in Singapore and in the US, where it is routed through cloud infrastructure operated by Oracle, an American company. In 2023, TikTok opened a data center in Ireland where it handles EU citizen data. These measures have not been sufficient for many US lawmakers, and in March 2023 the TikTok CEO, Shou Zi Chew, was called before Congress, where he faced more than five hours of intensive questioning about these and other practices. Lawmakers asked Chew about his own nationality, accusing him of fealty to China. He is, in fact, Singaporean. Various efforts to police TikTok and how it engages with US user data have been floated in Congress in the past year, culminating in the bill passed on Wednesday. Is this bill really a TikTok ban? Under the new bill, ByteDance would have 165 days to divest from TikTok, meaning it would have to sell the social media platform to a company not based in China. If it did not, app stores including the Apple App Store and Google Play would be legally barred from hosting TikTok or providing web-hosting services to ByteDance-controlled applications. Authors of the bill have argued it does not constitute a ban, as it gives ByteDance the opportunity to sell TikTok and avoid being blocked in the US. “TikTok could live on and people could do whatever they want on it provided there is that separation,” said Representative Mike Gallagher, the Republican chair of the House select China committee. “It is not a ban – think of this as a surgery designed to remove the tumor and thereby save the patient in the process.” TikTok has argued otherwise, stating it is not clear whether China would approve a sale or that it could even complete a sale within six months. “This legislation has a predetermined outcome: a total ban of TikTok in the United States,” the company said after the committee vote. “The government is attempting to strip 170 million Americans of their constitutional right to free expression. This will damage millions of businesses, deny artists an audience, and destroy the livelihoods of countless creators across the country.” How did we get here? TikTok has faced a number of bans and attempted bans in recent years, starting with an executive order by Donald Trump in 2020, which was ultimately blocked by courts on first amendment grounds. Trump has since reversed his stance, now opposing a ban on TikTok. Joe Biden, by contrast, has said he will sign the bill if it reaches his desk. Montana attempted to impose a statewide ban on the app in 2023, but the law was struck down by a federal judge over first amendment violations. The app was banned on government-issued phones in the US in 2022, and as of 2023 at least 34 other states have banned TikTok from government devices. At least 50 universities in the US have also banned TikTok from on-campus wifi and university-owned computers. The treasury-led Committee on Foreign Investment in the United States (CFIUS) in March 2023 demanded ByteDance sell its TikTok shares or face the possibility of the app being banned, Reuters reported, but no action has been taken. TikTok was banned in India in 2020 after a wave of dangerous “challenges” led to the deaths of some users. The ban had a marked effect on competition in India, handing a significant market to YouTube’s Shorts and Instagram Reels, direct competitors of TikTok. The app is not available in China itself, where Douyin, a separate app from parent company ByteDance with firmer moderation, is widely used. How would a ban on TikTok be enforced? Due to the decentralized nature of the internet, enforcing a ban would be complex. The bill passed by the House would penalize app stores daily for making TikTok available for download, but for users who already have the app on their phones, it would be difficult to stop individual use. Internet service providers could also be forced to block IP addresses associated with TikTok, but such practices can be easily evaded on computer browsers by using a VPN, or virtual private network, which re-routes computer connections to other locations. To fully limit access to TikTok, the US government would have to employ methods used by countries like Iran and China, which structure their internet in a way that makes content restrictions more easily enforceable. Who supports the potential TikTok ban? While Trump – who started the war on TikTok in 2020 – has reversed his stance on the potential ban, most Republican lawmakers have expressed support of it. The Biden administration has also backed the bill, with the press secretary, Karine Jean-Pierre, saying the administration wants “to see this bill get done so it can get to the president’s desk”. Biden’s campaign joined TikTok last month. Despite Trump’s opposition to the bill, many Republicans are pushing forward with the effort to ban TikTok or force its sale to an American company. “Well, he’s wrong. And by the way, he had his own executive orders and his own actions he was doing, and now … he’s suddenly flipped around on that,” said the representative Chip Roy, a Texas Republican and member of the far-right Freedom caucus. “I mean, it’s not the first or last time that I’ll disagree with the former president. The TikTok issue is pretty straightforward.” Who opposes the TikTok bill? TikTok has vocally opposed the legislation, urging the Senate not to pass it. “We are hopeful that the Senate will consider the facts, listen to their constituents, and realize the impact on the economy, 7m small businesses, and the 170 million Americans who use our service,” Alex Haurek, the TikTok spokesperson, said following Wednesday’s vote. Within the House, 50 Democrats and 15 Republicans voted against the bill, including the Republican representative Marjorie Taylor Greene of Georgia, who cited her experiences of being banned from social media. House Democrats including Maxwell Frost of Florida and Delia Ramirez of Illinois joined TikTok creators outside the Capitol following the vote to express opposition to the bill. Some Senate Democrats have already publicly opposed the bill, citing freedom of speech concerns, and suggested measures that would address concerns of foreign influence across social media without targeting TikTok specifically. “We need curbs on social media, but we need those curbs to apply across the board,” Senator Elizabeth Warren said. The Senate majority leader, Chuck Schumer, issued a neutral statement regarding the next steps in the Senate, stating it will “review the legislation when it comes over from the House”. Freedom of speech and civil rights advocacy groups have vehemently opposed a ban, stating that such legislation could have a profound impact on the internet at large. They have argued that TikTok’s data practices, while problematic, are not substantially different from those of US-based tech firms. “TikTok isn’t perfect, but banning it is the wrong solution,” said Jenna Ruddock, policy counsel at the media advocacy group Free Press. “Like all popular platforms, including those that Meta and Google own, TikTok collects too much data on its users. But unilaterally dismantling spaces for free expression limits people’s access to information and cuts off avenues for creators to build community.” What will happen to TikTok next? The bill still faces an uphill battle to become law. While Biden has confirmed he would sign it, it still has to pass a Senate vote. It’s unclear when that vote would take place, but TikTok is likely to increase its lobbying efforts on the Hill as it moves forward, with CEO Shou Zi Chew heading to Congress on Wednesday to speak with senators. Even if the bill were to pass, it is likely to face similar challenges on the grounds of free speech that prevented similar legislation – like that from Trump in 2020 and the Montana ban of 2023 – from moving forward.",
        "author": "Kari Paul",
        "published_date": "2024-04-24T22:45:34+00:00"
    },
    {
        "id": "8f12a212-bc17-4b5b-9f72-7ae297656e4a",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/24/meta-earnings",
        "title": "Meta says revenue will be weak as it spends even more on AI",
        "content": "Meta’s drive to integrate artificial intelligence into its products yielded strong financial results for the second quarter in a row. The company plans to spend even more on AI in the coming months, though, and its share price slumped more than 15% as the company reported earnings on Wednesday. A weak sales forecast and higher spending guidance rattled investors. Mark Zuckerberg said his main focus for the rest of 2024 and probably 2025 would be “getting millions or billions of people to use Meta AI” rather than generating revenue from it. He cautioned that Meta products such as Instagram Reels had in the past not immediately generated revenue as they scaled before they became profitable pieces of Meta’s ad business. He made the same prediction for Meta AI: “We build out the inventory first and then we monetize it,” saying Meta was “scaling the product before it is making money”. “We’re more optimistic and ambitious on AI” than last quarter, Zuckerberg added. Revenue at the world’s largest social media business increased 27% to $36.46bn during the first quarter in contrast to analyst expectations of $36.16bn. Earnings per share more than doubled to $4.71, surpassing expectations on Wall Street of $4.32. Shareholders were spooked by revenue guidance for the next quarter: $37.75bn, growth of about 18%. Analysts had predicted about $38.3bn. Forrester’s VP research director, Mike Proulx, said: “There’s no doubt that Meta is all-in on AI, but to achieve its vision, the company has to make big investments in infrastructure. Mark Zuckerberg’s ‘heads up’ was reminiscent of what he once said about the metaverse, and that didn’t exactly go so well. But this is different than Meta’s metaverse gamble because AI has real and practical use cases now.” Meta also now expects its capital expenditure to be between $30bn and $40bn this year, up from an earlier forecast of between $35bn and $37bn. Shares in the firm, which owns Facebook, Instagram and Whatsapp, were down sharply in after-hours trading. The group did not issue a dividend to investors for the first quarter of 2024, having done so for the first time in its history in the last quarter, which bolstered investors’ confidence. Proulx said: “The question remains whether Meta can contend in the AI race while maintaining a strong financial position. To do this, expect to see more ‘metaverse’ resources diverted from Reality Labs to Meta’s AI initiatives.” TikTok, a key competitor, is facing extreme regulatory pressure and uncertainty, leading investors to put their money in safer bets. The day before Meta reported earnings, the Senate approved a bill that would ban TikTok if it were not sold to an American company. Joe Biden signed the bill the day of the earnings report. Meta’s chief financial officer, Susan Li, said it was “too early to assess” the effect of the bill on Meta’s business. Meta’s shares have rallied by nearly 40% this year. Investors have cheered Zuckerberg’s and the company’s pivot from investment in its metaverse products to spending big on artificial intelligence. The Emarketer senior analyst Max Willens said the results were positive, but Meta needed to continue its upward momentum in the face of rising costs. “For all the recent scrutiny of its effectiveness, Meta’s ad business is humming,” Willens said. “With inventory levels and average price per ad rising, Meta is back to delivering enviable margins even as it continues to invest in opportunities that may be years away from contributing to profits.” Last week, the company announced it would integrate its partially released Llama 3 large language model across its platforms. The Meta AI assistant will also roll out to Australia, Canada, Singapore, Nigeria and Pakistan. The company also said it would add real-time image generation to its social networks. Investors also gave their blessing to Zuckerberg’s declaration of a “year of efficiency” in 2023, which in practical terms meant four sweeping rounds of layoffs. According to the earnings report, the company’s workforce was down 10% since last year. The company cut more than 20,000 jobs, reducing its headcount to a figure similar to that of 2021. The company had doubled its workforce from 2020. Regulatory headwinds have formed a dark cloud on the horizon, however. Zuckerberg testified at a congressional hearing earlier this year, during which lawmakers raised the prospect of stripping Meta of its primary shield from legal liability for what its users post. Advocates say Meta fails to adequately protect children, and some lawmakers agree. New Mexico’s attorney general is suing the company for allegedly facilitating child predation. More than 30 states are suing the company for allegedly endangering teens’ mental health. Speaking to parents at the hearing whose children had killed themselves after facing harassment and sex trafficking via social media, Zuckerberg said: “I’m sorry for all that you have been through.”",
        "author": "Blake Montgomery",
        "published_date": "2024-04-24T22:41:30+00:00"
    },
    {
        "id": "273709d6-aa36-4ce1-9b32-a4328ff46c77",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/24/tiktok-reward-to-watch-feature-suspended-after-eu-threats-to-block-it",
        "title": "TikTok reward-to-watch feature suspended after EU threats to block it",
        "content": "A TikTok service offering rewards such as gift vouchers for watching videos has been suspended by the company shortly after the EU threatened to block it amid fears of addiction among children. On Monday the digital commissioner, Thierry Breton, said the Chinese-owned video-sharing platform had “failed to prove” the feature on TikTok Lite, which launched recently in France and Spain, complied with obligations under sweeping new Digital Service Act (DSA) laws. He said the EU believed the service could be as “addictive as cigarettes” and gave the company 48 hours to respond with any fresh defence. In a pre-emptive move, TikTok wrote to Breton to say it would suspend the service. Breton said on Wednesday: “Our children are not guinea pigs for social media. The DSA ensures the safety of our EU online space.” It is the first non-compliance case taken by the EU since the DSA came into force in August last year. It is the latest blow to hit the company, after the US Senate voted on Tuesday to pass a bill that will either ban TikTok or force a sale of its US business. Breton expressed dismay that the company had gone ahead with the service knowing that the European Commission had already raised concerns with TikTok over general safeguarding issues relating to children. “I take note of TikTok’s decision to suspend the TikTok Lite ‘reward programme’ in the EU, following the opening of our case on 22 April and the communication of our readiness to adopt interim measures,” he said. “Our cases against TikTok on the risk of addictiveness of the platform continue, including the investigation to establish whether the launch of TikTok Lite was done in compliance with the DSA. “We suspect that this feature could generate addiction and that TikTok did not do a diligent risk assessment and take effective mitigation measures prior to its launch.” TikTok, owned by the Chinese company ByteDance, will suspend the “task and reward programme” service in France and Spain for 60 days for new users as of Wednesday. It will also stop providing the service to existing subscribers by no later than 1 May and pause the rollout in other EU countries. The suspension of the service is the first example of the EU using the powers under the DSA, which forces social media companies including X and Facebook to comply with EU laws or risk sanctions, which can include ban or fines of up to 6% of global revenues. A spokesperson for the European Commission confirmed the two formal proceedings into TikTok remained active, including this recent case regarding TikTok Lite. The first case was launched in February and concerns safeguarding of children on TikTok, including the issues of age verification, advertising transparency and the risk management of addictive design and harmful content. A TikTok spokesperson said: “TikTok always seeks to engage constructively with the EU Commission and other regulators. “We are therefore voluntarily suspending the rewards functions in TikTok Lite while we address the concerns that they have raised”",
        "author": "Lisa O'Carroll",
        "published_date": "2024-04-24T16:59:25+00:00"
    },
    {
        "id": "73d2d9ac-3826-4603-b127-5ba2a3bd0de1",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/24/why-is-us-threatening-to-ban-tiktok-and-could-other-countries-follow-suit",
        "title": "Why is US threatening to ban TikTok and will other countries follow suit?",
        "content": "Joe Biden has signed into law a bill that requires TikTok’s Chinese owner to sell the social media app’s US operations or face a ban, after the Senate passed the legislation. The law, part of a foreign aid package for Ukraine, Israel and Taiwan, sets the clock ticking on a potential ban for a platform that is hugely popular in the US. Here is a guide to the TikTok legislation and what may happen next. How does the legislation pave the way for a sale or ban? The bill gives TikTok’s Beijing-based parent, ByteDance, 270 days to sell the app’s US operations. If ByteDance appears close to completing a deal as the deadline looms, the president can authorise a 90-day extension. The deadline arrives at about the time the next president is inaugurated, on 20 January, which means Donald Trump, if he wins the election, could decide whether the sale process is extended. If ByteDance fails to carry out a sale, then it will face a nationwide ban, by the blocking of app stores and web hosts from distributing TikTok. Why is the US threatening to ban TikTok? US lawmakers and authorities are concerned that data from TikTok’s 170 million US users could be accessed by the Chinese state under national security laws. Christopher Wray, the director of the FBI, the US domestic intelligence and security agency, has said that ByteDance is “controlled by the Chinese government” and has warned that Beijing authorities can influence people by manipulating the algorithm that curates what people view on TikTok, as well as allowing the government to collect user data for “traditional espionage operations”. TikTok denies that the Chinese government has attempted to access US user data and says it would reject any such request. In an appearance before Congress last year, TikTok’s chief executive, Shou Zi Chew, said: “Let me state this unequivocally: ByteDance is not an agent of China or any other country.” Will TikTok appeal against the law? TikTok has already declared that it will fight the bill in the courts as soon as it is signed, labelling it a breach of the US constitution’s first amendment, which protects free speech. “At the stage that the bill is signed, we will move to the courts for a legal challenge,” wrote TikTok’s head of public policy for the Americas, Michael Beckerman, in a memo to staff at the weekend. He added: “We’ll continue to fight, as this legislation is a clear violation of the first amendment rights of the 170 million Americans on TikTok.” Chew said in a video posted soon after Biden signed the bill on Wednesday that “we aren’t going anywhere”, adding that “the facts and the constitution are on our side and we expect to prevail again”. The first amendment stance has already worked in TikTok’s favour after a judge in Montana, which had banned the app, blocked the move because it violated users’ free speech rights. The last time the US tried to ban TikTok, in 2020 after an executive order issued by Trump, the company secured a preliminary injunction against the move after a judge in Washington said a prohibition may “likely exceed” the bounds of the law. TikTok could pursue an injunction again, before challenging the constitutionality of the bill in a full case. Carl Tobias, a professor at the University of Richmond law school, believes the legal process will probably take two years “during which time the law would not ban the app”. The US government is likely to argue in court that there are national security grounds for a ban. David Greene, civil liberties director at the Electric Frontier Foundation, a digital rights group, said: “The government will have to demonstrate to the court that the national security concern is real and not hypothetical or conjectural … and that a total ban on TikTok as it currently exists is the appropriately tailored means of addressing that national security concern.” Who might be interested in buying TikTok’s US operations? The former US treasury secretary, Steve Mnuchin, said in March he was putting together a consortium to buy TikTok’s US assets, calling it a “great business”. If past suitors are any indication of interest, in 2020 Microsoft explored a deal to buy TikTok, at the behest of Trump, who had also encouraged the US tech firm Oracle and the retail corporation Walmart to take a large stake. ByteDance has a number of US investors, including the investment firms General Atlantic, Susquehanna and Sequoia Capital. However, analysts at Wedbush Securities, a US financial services firm, say they do not expect the Chinese government to sanction a sale that includes TikTok’s algorithm, the extremely effective technology that curates what people see on the app. “The value of TikTok would dramatically change without the algorithms and makes the ultimate sale/divestiture of TikTok a very complex endeavour with many potential strategic/financial bidders waiting anxiously for this process to kick off,” Wedbush said in a note to investors, which valued its UK operations at $100bn (£80.3bn) with the algorithm but between $30bn and $40bn without. What does the Chinese government think? The Chinese government said last year it would “firmly oppose” the sale of the app, adding it would “seriously undermine the confidence of investors from various countries, including China, to invest in the United States”. China also has export rules that prohibit the sale of certain technologies. Will other countries follow suit with a divest-or-ban move? TikTok is already under pressure elsewhere in the west because of shared concerns over data. It has been banned from government-issued phones in the UK, the US, Canada and New Zealand – and staff at the European Commission have also been banned from using it on work-issued devices. Calls for a UK ban have been voiced, including by the former Conservative party leader Iain Duncan Smith, who said last month “we should have done it ourselves”. TikTok and dozens of other Chinese apps were banned in India in 2020 after the government said they were “prejudicial to sovereignty and integrity of India, defence of India, security of state and public order”.",
        "author": "Dan Milmo",
        "published_date": "2024-04-24T16:42:38+00:00"
    },
    {
        "id": "4f0ee3a5-7c1c-4113-9fc7-06b15efed8cb",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/24/uk-competition-watchdog-steps-up-scrutiny-of-big-techs-role-in-ai-startups-cma-microsoft-amazon",
        "title": "UK competition watchdog steps up scrutiny of big tech’s role in AI startups",
        "content": "The UK competition watchdog has stepped up its scrutiny of big tech involvement in artificial intelligence startups, asking for comment on three deals by Microsoft and Amazon. The Competition and Markets Authority (CMA) announced that it was examining Microsoft’s investment in the French firm Mistral and the hiring of the DeepMind co-founder Mustafa Suleyman as head of the US company’s new AI division. The watchdog is also scrutinising Amazon’s $4bn (£3.2bn) investment in the US AI firm Anthropic. The CMA has issued “invitations to comment” on the tie-ups, a procedural move that paves the way for a formal investigation, amid concerns that these partnerships are effectively giving the big tech companies backdoor control over potential rivals and stifling competition. The watchdog has already asked for comments on Microsoft’s relationship with OpenAI, the developer of ChatGPT. Joel Bamford, the executive director of mergers at the CMA, said: “We will assess, objectively and impartially, whether each of these three deals fall within UK merger rules and, if they do, whether they have any impact on competition in the UK.” Meanwhile, Sarah Cardell, the chief executive of the CMA, has warned that the watchdog had “real concerns” about the AI market. The CMA has singled out six tech companies at the heart of an “interconnected web” of AI partnerships: Google; Microsoft; Meta; Amazon; Apple; and Nvidia, the leading supplier of chips for training and operating AI systems. The CMA’s next step with Microsoft and Amazon will be a “phase one” investigation, during which the watchdog will examine whether the partnerships fall under the UK merger regime and whether they raise competition concerns. If the CMA finds there are concerns and decides to proceed with a phase two investigation, it could seek remedies from the companies involved. In February, Microsoft announced it was investing €15m (£13m) in Mistral, a Paris-based startup specialising in open-source AI models, which can be freely downloaded and adapted by users. Microsoft said the deal represented an opportunity for Mistral to “unlock new commercial opportunities” and “expand to global markets”. The Mistral deal was followed weeks later by the announcement of Suleyman’s appointment. Several employees at his Inflection AI startup were also recruited to join the new Microsoft division. The tie-up included a $650m cash payment to Inflection and making the company’s AI models available on Microsoft’s Azure cloud service. Amazon’s deal with Anthropic, the developer of the Claude chatbot, involves the US startup using the big tech company as its main cloud provider and using Amazon’s custom chips to build, train and deploy AI models. Alex Haffner, a partner at the UK law firm Fladgate, said the move underlined the CMA’s interest in the AI market. “Noises coming out of the CMA in recent times have indicated that they are very keen to get involved early in the evolution of the AI market and today’s announcement certainly reflects that degree of seriousness,” he said. Microsoft said it would provide the CMA with the information the watchdog needed to carry out its inquiries. “We remain confident that common business practices such as the hiring of talent or making a fractional investment in an AI startup promote competition and are not the same as a merger,” the company said. Amazon said the CMA move was “unprecedented” and that the Anthropic deal was different from other tie-ups announced by the big tech firms. In a pointed reference to Microsoft’s partnership with OpenAI, Amazon said the deal did not give it a “board director or observer role” and continued to have “Anthropic running its models on multiple cloud providers”.",
        "author": "Dan Milmo",
        "published_date": "2024-04-24T15:26:47+00:00"
    },
    {
        "id": "aaa32115-1e09-4d87-87c4-c8b2d6c8b0b2",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/24/tell-us-whats-your-favourite-everyday-gadget",
        "title": "Tell us: what’s your favourite everyday gadget?",
        "content": "What’s your favourite, most useful everyday gadget? It could be a much-used kitchen gizmo, a tool for your daily beauty routine that you can’t live without, or a piece of kit that makes your day-to-day life easier: anything small, genuinely useful, and inexpensive to buy (nothing over £20). This Community callout closed on 25 June 2024. You can see the article that included respondents to this callout here. You can contribute to open Community callouts here or Share a story here.",
        "author": "Guardian community team",
        "published_date": "2024-04-24T14:40:09+00:00"
    },
    {
        "id": "754380b1-ff4c-47d1-bee3-2c4caa775f37",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/24/how-soon-can-tesla-get-its-more-affordable-car-to-the-market",
        "title": "How soon can Tesla get its more affordable car to market?",
        "content": "Tesla’s plans to bring a more affordable electric vehicle to the market appear to have moved a step closer. On Tuesday, the company’s share price shot up by 12% after an update revealed the carmaker was hoping to accelerate the production of lower-priced EVs, with production of the first cars beginning as early as this year. So, with investors clearly excited, what exactly is Tesla planning and how soon will it reach the market? What is Tesla planning? The key change to its approach is to go faster on the start of production. On Tuesday, the company said it had updated its future vehicle lineup “to accelerate the launch of new models ahead of our previously communicated start of production in the second half of 2025”. These would include new “more affordable” vehicles, it added. In a call with investors, the Tesla boss Elon Musk clarified that this would probably mean production starting “by early 2025, if not late this year”. But while the company was clear on timelines, it gave very few details on the design and specifications of the new cars. Previously Tesla had talked about building an unnamed vehicle, often referred to as the Model 2, which would be priced at $25,000 (£20,000) or less. However, it was reported this month that plans to produce the car had been scrapped. During the investor call, Musk appeared reticent when asked about whether the new vehicles would be all-new, or tweaks to existing models. How can it achieve this? To accelerate the production of the new, more affordable cars, it would seem Tesla is having to retreat, at least partly, from the Model 2 plan. The vehicle would have required a move away from the traditional assembly lines. Tesla wanted to roll out its “Unboxed” manufacturing process, which would have different parts of the car built by large subassembly groups at the same time, and then brought together and connected at the end. It was hoped this would halve production costs. But it would also require billions of pounds in investment to get factories ready. However, Tuesday’s announcement indicated the new vehicles would be built on current manufacturing lines. According to Reuters, Tesla’s engineering chief Lars Moravy said the company would avoid the risk of investing in a “revolutionary” manufacturing process. Some commentators believe this marks a complete shelving of the Model 2 plan. Sam Abuelsamid, an analyst at the consultancy firm Guidehouse Insights, said: “The next generation vehicle was supposed to use fundamentally different production processes from current models. “With no desire to spend billions on new production facilities or retool existing factories, it seems like we will see Tesla continue to build the current products.” How much will the more affordable car cost? The cost of the new “more affordable” models is still unclear. Thecheapest of the current Model 3 vehicles has a starting price of about $39,000. Tesla had initially hoped that the new affordable model would be able to compete with Chinese rivals by making its Model 2 cost $25,000. However, the company cautioned that its revised plan “may result in achieving less cost reduction than previously expected”.",
        "author": "Jack Simpson",
        "published_date": "2024-04-24T12:02:40+00:00"
    },
    {
        "id": "70bcb4b7-357e-4587-a214-552486203969",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/24/harry-daniels-tiktok-singing-celebrities",
        "title": "‘I may be a troll but I’m not stupid’: super-stan Harry Daniels on singing loudly at Biden, Dua Lipa and Anna Wintour for clout",
        "content": "Billie Eilish has run from him. Doja Cat stopped her security detail to allow for a sidewalk serenade of Paint the Town Red. Charli XCX let him sing a few bars from I Got It before telling him “You need to work on it,” turning on her heel, and strutting back to her car. Harry Daniels stakes out celebrities such as Dua Lipa, Katy Perry, Ellie Goulding – and, uh, Joe Biden – and serenades them while filming their responses for TikTok. Most of these interactions appear spontaneous, as if the celebrities are genuinely surprised to be accosted by a 20-year-old man singing at them, usually terribly and oftentimes with their own songs. When Daniels found Jacob Elordi at a restaurant, the Saltburn star stayed across the room next to a bodyguard-type, looking amused but slightly wary as Daniels crooned Murder on the Dancefloor his way. Daniels’ videos alternate between old-fashioned trolling (Daniels once thanked Lea Michele for “all you do for the community of people who can’t read”) and displays of genuine love for an artist. In an era when celebrities maintain strict control over their images, his improv manages to reveal whether there is a sense of humor, a lick of personality, or anything at all going on beneath the surface. To quote a popular genre of comments Daniels receives on TikTok, “HOW DOES FIND THESE PEOPLE?!?!?” Daniels has long cultivated an online obsession with pop princesses. “I think there’s a big element of escapism to stan culture,” he told me when I met him and his sister Madeline Daniels at a Lower East Side cafe in New York last week. “I found solace in other people’s careers and work, because oftentimes I felt like I was insecure in my own life.” Daniels grew up in Long Island, the son of an accountant and stay-at-home mom. “Harry was around 11 when he first enrolled in stan university, where he has a PhD,” said Madeline, who acts as his sometimes camerawoman and manager. He loved Demi Lovato, Billie Eilish, Fifth Harmony and Haim, and engaging with their fandoms on social media taught him how to follow his faves in real life. Two years ago, Daniels started using his stanning education to wrangle face time with superstars. “I was going to everyone’s shows and meet-and-greets and engaging with them to feel like I had a sense of connection with them,” he said. But he found the interactions too formulaic. “How many times am I going to tell someone, ‘I love you so much?’ I wanted to make a lasting impression so I thought, fuck it, I’ll be a troll and be entertaining.” Daniels filmed his first TikTok serenade at a Sabrina Carpenter album signing in 2022. In the video, he sings the nascent pop star’s diss track Skin while she sits behind a table, clearly holding in nervous laughter. As soon as Daniels finishes, Carpenter lets out a very diplomatic “thank you”. At that moment, a shtick was born. Daniels soon became “more clever” than meet-and-greets. He waited outside a Broadway stage door to land Sarah Paulson. Sometimes he got lucky: while eating at a diner one night, he ran into Ethan Cutkosky, who played Carl on the US version of Shameless. “I think people think I’m like, hiding in bushes, but I really just show up places I know they’ll be,” he said. (It helps that he lives just outside New York City.) Eventually, Daniels reached a level of virality such that some artists’ teams started reaching out: The music industry’s overreliance on TikTok as a promotional tool means labels are more likely to back an artist who can generate buzz on the app. A smartly planned “surprise” run-in with Daniels, who has over 1 million followers, is catnip for artists looking to go viral or curate relatable online personas. Lipa’s team invited Daniels to surprise their star during press tours. Likewise, Daniels went to Coachella earlier this month with backstage access that allowed him to sing Taylor Swift’s Anti-Hero to Jack Antonoff, and Paris Hilton’s Stars Are Blind to the heiress herself. Daniels says that a lot of his videos “are just for fun”, meaning he doesn’t get paid. The money comes in when a sponsor pays him to film with artists at an event. (Daniels declined to reveal the most he’d been paid for an appearance.) According to Daniels, an artists’ team might know what’s going on, but the artist “legit has no idea”, which he claims helps the manufactured situation retain its authenticity. “But lately, people have been trying to tell me how to do my own content.” He’s had artists ask him to promote their new song or name-drop a product. “I’ve had to put my foot down, because I want my videos to be more culturally relevant than commercially blah, blah, blah,” he said. Daniels prefers to stick in the pop culture realm, though he recently filmed a video with Biden after receiving an invitation to his star-studded Radio City Music Hall fundraiser in March. Daniels was able to get close to the barricade where Biden walked around shaking attendees’ hands so he could sing Lana del Rey’s National Anthem to the 81-year-old politician. “He was very present, but I think he was just like, literally what the fuck?” Daniels said. Daniels would sing to Kamala Harris too, given the chance. What song? The Wheels on the Bus, of course, alluding to the vice-president’s apparent love for buses, loudly laughing, and loudly laughing about buses. But we shouldn’t expect a Trump video. “I don’t think I’ll ever get that close to him,” Daniels said. Not everyone appreciates the Harry Daniels treatment. At the Biden fundraiser after-party, Daniels tried and failed to sing to the inimitable Vogue editor-in-chief Anna Wintour: “I knew going into it that Anna doesn’t do that. I may be a troll, but I’m not stupid,” Daniels said. “So I introduced myself, said that I’m a fan, and she was like, ‘Thank you, you’re so kind.’ She just walked away.” Daniels also got some flak after asking America Ferrera if she’d prefer having “a gay son or a thot daughter” at the People’s Choice awards; his critics accused him of being “not funny” and “not a real journalist”. He shrugged it off, telling Rolling Stone, “My job is to create content that will generate clicks and views.” Daniels has an ulterior motive for his posts. As he first told Rolling Stone, he’s a singer himself, and not a bad one at that. The magazine described his vocals as possessing a “similar innate melancholy as Troye Sivan’s”. “The only reason I really started all of this was because I wanted to pursue music, and basically everyone I’ve ever spoke to in the business consistently told me I needed to have a following on social media,” Daniels said. “So if people want a following, I’ll give them a following.” Daniels doesn’t seem annoyed by this give and take; he respects the hustle. Still, there’s something bleak about his outsider-y bit inevitably becoming part of the star machine. The more celebs get tipped off about a potential Harry Daniels interaction, the more the videos feel like SNL shorts – less gonzo fun, more fuel for PR campaigns and album release calendars. Regardless, Daniels says “the music comes first”. He describes his work as hyperpop, the bubblegum-meets-chaos genre typified by Charli XCX and Kim Petras. “It’s larger than life, and reminds me what music should be: it’s supposed to soundtrack something bigger than you, and make you feel things you can’t put into words,” he said. He hasn’t released any tracks yet, but says 2024 will be the last year for his current shtick. It’s not that he’s over it; he knows he’ll have to create content forever if he wants to stay relevant, and that’s fine with him. He just sees an opportunity to “strike when the iron is hot”. He’s currently in talks to expand his homespun team to include professionals who aren’t just his sister. As Daniels left the cafe, one young woman and apparent fan standing near the exit quietly said, “I love you.” Daniels cooed back a breathy, saccharine, “I love you, too” – the sort of delivery I bet he picked up from the celebs he’s sung to.",
        "author": "Alaina Demopoulos",
        "published_date": "2024-04-24T10:00:38+00:00"
    },
    {
        "id": "23f5990e-69e4-4d63-b0cb-8bdc69ad0493",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/23/senate-passes-bill-banning-tiktok-will-parent-company-sell",
        "title": "Senate passes bill banning TikTok if parent company does not sell it",
        "content": "The Senate voted on Tuesday to pass a bill that will either ban TikTok or force a sale of the short-form video app, giving its China-based parent company ByteDance up to one year to divest its crown jewel before facing deletion from US app stores. The vote was a landslide, with 79 senators voting in favor and 18 against. The bill passed in the House on Saturday by a margin of 360 to 58, as part of a foreign aid package for Ukraine, Israel and Taiwan. It will now make its way to the desk of Joe Biden, who has previously said he would sign the legislation. The new law gives ByteDance a year to sell TikTok to a company that is not controlled by a “foreign adversary”, or the app will face a total ban from American app stores. TikTok’s head of public policy for the Americas, Michael Beckerman, made it clear in that time, the company will fight the legislation in court. If the new federal law goes into effect without being blocked, Apple’s App Store and Google’s Play Store will be required to stop offering TikTok for download or face financial penalties. “At the stage that the bill is signed, we will move to the courts for a legal challenge,” Beckerman wrote in the memo, which was first reported by the tech news website the Information. Beckerman claimed that the bill violated the first amendment of the US constitution, which protects freedom of speech. “We’ll continue to fight, as this legislation is a clear violation of the first amendment rights of the 170 million Americans on TikTok,” he wrote. State-level attacks on TikTok have been successfully defeated on similar grounds in the US. In 2023, a federal judge in Montana blocked the state’s ban on TikTok, saying the prohibition violated the free speech rights of users. The judge wrote that the law “oversteps state power and infringes on the constitutional rights of users”. The passage of the ban is the culmination of a years-long political battle over the platform, which exploded in popularity after its emergence in 2017. Donald Trump announced he would ban it in 2020, though his efforts to do so did not bear fruit. More recently, he has railed against the possibility of a ban and said Biden would be “responsible” for such a measure. Lawmakers have argued that TikTok’s China-based parent company could collect sensitive user data and censor content that goes against the Chinese government – claims TikTok denies. Multiple contentious congressional hearings have covered what TikTok’s data privacy practices are. Still, such concerns have resulted in bans across a number of college campuses, political offices and individual states. TikTok was banned in India in 2020 after a wave of dangerous “challenges” led to the deaths of some users, and the app is not available in China itself. • This article was amended on 10 May 2024. An earlier version stated that the bill means ByteDance is required to sell TikTok to a US-based company. It is in fact required to sell the platform to a buyer that is not controlled by a “foreign adversary”, which includes China.",
        "author": "Kari Paul",
        "published_date": "2024-04-24T01:58:54+00:00"
    },
    {
        "id": "0522fb7b-738c-4b90-aa85-dfb897347cb1",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/23/tesla-tsla-earnings-report-cybertruck",
        "title": "Tesla sees biggest revenue drop since 2012 but company shares still surge",
        "content": "Tesla shares surged nearly 10% in after-hours trading on Tuesday after posting earnings results, despite a revenue miss for the first quarter of 2024, a steep decline in profits, and a recall of its most recently released car, the $100,000 Cybertruck. The electric vehicle manufacturer posted $21.3bn in revenue, lower than the $21.48bn that was estimated and a 9% drop year over year – marking its biggest decline since 2012. Profit was $1.1bn, a 55% decline from the first quarter of 2023, the company said. Still, the report offered heartening announcements for investors, including previews of a ride-hailing app to be integrated into Tesla products. The company said it expected to release new vehicle models sooner than previously announced and referenced a robotaxi network in the works. It has more than doubled its AI compute – the complexity of its smart software – in the past three months and invested $1bn on AI infrastructure during the same time period. Tuesday’s report – particularly Tesla’s plans to accelerate the development of lower-cost vehicles – has assuaged some of those fears, said Thomas Monteiro, senior analyst at Investing.com. “The announcement also indicates that Elon [Musk] may be focusing efforts on the EV giant again, which is another piece of good news for shareholders,” he said. “It’s a promising sign for the company that he felt compelled to change its direction amid the myriad of pressures.” The earnings report was the second since the launch of the Cybertruck, Tesla’s long-awaited electric pickup truck, and its first earnings call since that vehicle was recalled last week. The futuristic steel car has struggled with other malfunctions. Tesla voluntarily issued a recall following reports of the vehicles were at risk of getting stuck driving at full speed due to a loose accelerator pedal. The company did not comment on the recall directly in its earnings release. Even without the Cybertruck concerns, Tesla had been facing a difficult year, saying last week that it was cutting 10% of its staff globally, about 14,000 jobs. Over the weekend, it slashed its prices around the world. It has weathered a series of poor earnings reports in recent quarters as competing Chinese electric vehicle manufacturers encroach on the electric vehicle market. Last quarter, Tesla reported that vehicle deliveries had fallen for the first time in four years. In Tuesday’s report, Tesla said its vehicle volume growth rate “may be notably lower than growth rate achieved in 2023”. Musk continues to face criticisms from investors who say he is stretched too thin after his purchase of Twitter, which he renamed X, in late 2022. The company and its products have been plunged into chaos in the ensuing year and a half. Meanwhile, Tesla has asked its shareholders to vote in favor of Musk’s $56bn pay package that was rejected by a judge earlier this year who called it an “unfathomable sum”. Musk addressed those concerns directly on the earnings call, saying: “Tesla constitutes the majority of my work time. I work every day of the week. I’m going to make sure Tesla is very prosperous.”",
        "author": "Kari Paul",
        "published_date": "2024-04-24T00:06:23+00:00"
    },
    {
        "id": "6b73fab2-459e-4b20-a5e6-e49d178d9038",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/23/techscape-apple-app-store-china-tiktok-meta-llama",
        "title": "TechScape: No WhatsApp in China, no TikTok in the US, and the return of Llama",
        "content": "Another day, another set of troubles for Apple’s App Store. This time, the company had bowed to orders from the Chinese state to remove WhatsApp and Threads, two of the last Meta apps still available in the country. From our story: Apple confirmed it had withdrawn the two apps – both owned by Meta, also the owner of Facebook – under instruction from the Cyberspace Administration of China, which regulates and censors China’s highly restricted internet and online content. “The Cyberspace Administration of China ordered the removal of these apps from the China storefront based on their national security concerns,” Apple said in an emailed statement to Reuters. “We are obligated to follow the laws in the countries where we operate, even when we disagree.” The reader, perhaps, is intended to infer a final clause in that statement, and conclude that Apple does indeed disagree with the order. It seems unlikely that the company was overjoyed to bow to high-profile censorship yet again, even if it did involve taking down the apps of one of the company’s most formidable competitors. But it is worth noting what Apple hasn’t done this time: repeated its playbook with the European Union. The company hasn’t published a 12-page report detailing the changes it has been forced to make, and explaining at length why it thinks being forced to make them is likely to harm the experience for Apple customers and run counterproductive to the goals of the regulator. It hasn’t pursued a strategy of so-called “malicious compliance”, following the letter but not the spirit of the law. And it certainly hasn’t declared that its interpretation of the law differs from that of the regulators, and vowed to fight through the courts. It may be unfair to expect it to, of course. Fighting the Chinese state in the Chinese courts would be an exercise in futility, to the extent that it would even be possible. Nonetheless, the difference is stark. When an authoritarian regime tells Apple what it can do with the App Store, the company’s response is a curt single paragraph. When a democratic union tries to do the same, the response is vociferous and negative. The comparison does no favours to the European Union, to be sure. No defender of the Digital Markets Act would be too happy to stare Apple in the eye and demand to be treated like the Chinese Communist party. It’s an observation I feel compelled to make every time it comes up. There’s one world government that can command Apple with barely a squeak of public protest, and it’s not the one you’d want. Time’s up for TikTok China isn’t the only government that bans things. From the Guardian US: The House of Representatives voted 360 to 58 on the updated divest-or-ban [TikTok] bill that could lead to the first time ever that the US government has passed a law to shut down an entire social media platform. The Senate is expected to vote on the bill next week and Joe Biden has said he will sign the legislation. I’m not a legislator but the bill itself doesn’t seem like great law. To avoid the appearance of passing a bill of attainder (a piece of legislation specifically targeting an individual, generally looked down upon in the English legal tradition from which US governance descends), it covers TikTok and any other service “controlled by” a “foreign adversary” deemed to be a security risk at the determination of the president. That’s quite a lot of scope to ban things at will, given the looseness apparent in the claim that something “controlled by” the state. “Foreign adversary” is more strictly defined by other legislation, and currently covers Iran, China, Russia and North Korea. But that’s a debate for the lawyers. What interests me is: what will TikTok look like without Americans? The domination of the anglophone internet by the US is a reality. The “world wide web” isn’t a misnomer, and although there remain geographic differences in which services are popular where, as a general rule, American voices will dominate. It seems unlikely that American TikTok will be wiped out by any ban – the number of compulsive users willing to hack their devices, use web apps or simply never uninstall the service means there will be always be posters stateside. But any substantial friction will probably lead to lots of less diehard TikTok users jumping ship to Instagram, Snapchat and YouTube Shorts, all of which have benefited from TikTok restrictions in other regions, particularly India. In the short term, that can’t help but make the TikTok experience worse for all of its users, wherever they live. Some portion of the content they would have wanted to watch will no longer be there, and much of the rest will be in reposts, arriving late and shorn of any connection with their original creators. What I’m curious about, though, is the mid- to long-term outcome of such a change. Does the rest of the anglophone world steadily drop off as well, following the American users to the places they’ve started posting? Or is there a genuine divergence, as the culture on TikTok starts to reflect a fundamentally different cross-section of the world from the internet at large? I still think the most likely outcome is that we never find out, and some combination of money, lobbying and protest means that an arrangement is found that maintains access to TikTok. But if it all fails, what a fascinating experiment we’ll get to witness. Llama 3 on the loose The second summer of AI has begun. The smallest versions of Facebook’s large language model Llama 3 have been released, and they have changed the landscape for what is unlikely to be the last time this year. From our story: The social media giant equipped Llama 3 with new computer coding capabilities and fed it images as well as text this time, though for now the model will output only text, Chris Cox, Meta’s chief product officer, said in an interview. More advanced reasoning, like the ability to craft longer multi-step plans, will follow in subsequent versions, he added. Versions planned for release in the coming months will also be capable of “multimodality”, meaning they can generate both text and images. Llama is Facebook’s homegrown competitor to GPT, Gemini and Claude. Unlike those three systems, though, Llama has been released on a comparatively open licence, with the core models available for users to download. If you want to build your own AI system – particularly if you want to run it locally, rather than relying on a server farm – Llama is a good place to start. The first version of Llama was accidentally-on-purpose released to the public, while the second made it official. The upside for Facebook is obvious: if you make the core technology on which the AI boom rests, you have an awful lot of power. Llama is open enough to build on top of, but not open enough to fully fork and take away from Facebook – crucially, without the data and specific details of how the system was created, you can never re-do that initial training run, even if you could afford the vast expense in compute. But the downside is equally clear. Facebook makes money on people using its services, but can’t directly earn revenue from training Llama. That means that it’s not incentivised to invest quite as much in staying at the absolute frontier as its competitors, and as a result, Llama has historically been far from the cutting edge. For the time being, however, that’s not the case. It took more than a year, but a freely licensed model from Facebook is now at least competitive with GPT4, and the company says an even better – or, bigger, at least – version is still to come. Of course, the frontier doesn’t stand still for long. OpenAI is expected to release GPT-5 at some point this summer, and, if it represents the progression insiders have been hinting at, everything will change – again. If you want to read the complete version of the newsletter please subscribe to receive TechScape in your inbox every Tuesday.",
        "author": "Alex Hern",
        "published_date": "2024-04-23T10:50:57+00:00"
    },
    {
        "id": "dd2713e4-37ba-4e00-be0d-be0e1e05a3a1",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/23/can-ai-image-generators-be-policed-to-prevent-explicit-deepfakes-of-children",
        "title": "Can AI image generators be policed to prevent explicit deepfakes of children?",
        "content": "Child abusers are creating AI-generated “deepfakes” of their targets in order to blackmail them into filming their own abuse, beginning a cycle of sextortion that can last for years. Creating simulated child abuse imagery is illegal in the UK, and Labour and the Conservatives have aligned on the desire to ban all explicit AI-generated images of real people. But there is little global agreement on how the technology should be policed. Worse, no matter how strongly governments take action, the creation of more images will always be a press of a button away – explicit imagery is built into the foundations of AI image generation. In December, researchers at Stanford University made a disturbing discovery: buried among the billions of images making up one of the largest training sets for AI image generators was hundreds, maybe thousands, of instances of child sexual abuse material (CSAM). There may be many more. Laion (Large-scale AI Open Network), the dataset in question, contains about 5bn images. With half a second a picture, you could perhaps look at them all in a lifetime – if you’re young, fit and healthy and manage to do away with sleep. So the researchers had to scan the database automatically, matching questionable images with records kept by law enforcement, and teaching a system to look for similar photos before handing them straight to the authorities for review. In response, Laion’s creators pulled the dataset from download. They had never actually distributed the images in question, they noted, since the dataset was technically just a long list of URLs to pictures hosted elsewhere on the internet. Indeed, by the time the Stanford researchers ran their study, almost a third of the links were dead; how many of them in turn once contained CSAM is hard to tell. But the damage has already been done. Systems trained on Laion-5B, the specific dataset in question, are in regular use around the world, with the illicit training data indelibly burned into their neural networks. AI image generators can create explicit content, of adults and children, because they have seen it. Laion is unlikely to be alone. The dataset was produced as an “open source” product, put together by volunteers and released to the internet at large to power independent AI research. That, in turn, means it was widely used to train open source models, including Stable Diffusion, the image generator that, as one of the breakthrough releases of 2022, helped kickstart the artificial intelligence revolution. But it also meant that the entire dataset was available in the open, for anyone to explore and examine. The same is not true for Laion’s competition. OpenAI, for instance, provides only a “model card” for its Dall-E 3 system, which states that its pictures were “drawn from a combination of publicly available and licensed sources”. “We have made an effort to filter the most explicit content from the training data for the Dall-E 3 model,” the company says. Whether those efforts worked must be taken on trust. The vast difficulty in guaranteeing a completely clean dataset is one reason why organisations like OpenAI argue for such limitations in the first place. Unlike Stable Diffusion, it is impossible to download Dall-E 3 to run on your own hardware. Instead, every request must be sent through the company’s own systems. For most users, an added layer places ChatGPT in the middle, rewriting requests on the fly to provide more detail for the image generator to work with. That means OpenAI, and rivals such as Google with a similar approach, have extra tools to keep their generators clear: limiting which requests can be sent and filtering generated images before they are sent to the end user. AI safety experts say this is a less fragile way of approaching the problem than solely relying on a system that has been trained never to create such images. For “foundation models”, the most powerful, least constrained products of the AI revolution, it isn’t even clear that a fully clean set of training data is useful. An AI model that has never been shown explicit imagery may be unable to recognise it in the real world, for instance, or follow instructions about how to report it to the authorities. “We need to keep space for open source AI development,” said Kirsty Innes, the director of tech policy at Labour Together. “That could be where the best tools for fixing future harms lie.” In the short term, the focus of the proposed bans is largely on purpose-built tools. A policy paper co-authored by Innes suggested taking action only against the creators and hosts of single-purpose “nudification” tools. But in the longer term, the fight against explicit AI images will face similar questions to other difficulties in the space: how do you limit a system you do not fully understand?",
        "author": "Alex Hern",
        "published_date": "2024-04-22T23:01:04+00:00"
    },
    {
        "id": "79e607ce-2ccc-41d1-8a1a-147434ba2dee",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/23/paedophiles-create-nude-ai-images-of-children-to-extort-them-says-charity",
        "title": "Paedophiles create nude AI images of children to extort from them, says charity",
        "content": "Paedophiles are being urged to use artificial intelligence to create nude images of children to extort more extreme material from them, according to a child abuse charity. The Internet Watch Foundation (IWF) said a manual found on the dark web contained a section encouraging criminals to use “nudifying” tools to remove clothing from underwear shots sent by a child. The manipulated image could then be used against the child to blackmail them into sending more graphic content, the IWF said. “This is the first evidence we have seen that perpetrators are advising and encouraging each other to use AI technology for these ends,” said the IWF. The charity, which finds and removes child sexual abuse material online, warned last year of a rise in extortion cases where victims are manipulated into sending graphic images of themselves and are then threatened with the release of those images unless they hand over money. It also flagged the first examples of AI being used to create “astoundingly realistic” abuse content. The anonymous author of the online manual, which runs to nearly 200 pages, boasts about having “successfully blackmailed” 13-year-old girls into sending nude imagery online. The IWF said the document had been passed to the UK’s National Crime Agency. Last month the Guardian revealed that the Labour party was considering a ban on nudification tools that allow users to create images of people without their clothes on. The IWF has also said 2023 was “the most extreme year on record”. Its annual report said the organisation found more than 275,000 webpages containing child sexual abuse last year, the highest number recorded by the IWF, with a record amount of “category A” material, which can include the most severe imagery including rape, sadism and bestiality. The IWF said more than 62,000 pages contained category A content, compared with 51,000 in the prior year. The IWF found 2,401 images of self-generated child sexual abuse material – where victims are manipulated or threatened into recording abuse of themselves – taken by children aged between three and six years old. Analysts said they had seen abuse taking place in domestic settings including bedrooms and kitchens. Susie Hargreaves, the chief executive of the IWF, said opportunistic criminals trying to manipulate children were “not a distant threat”. She said: “If children under six are being targeted like this, we need to be having age-appropriate conversations now to make sure they know how to spot the dangers.” Hargreaves added that the Online Safety Act, which became law last year and imposes a duty of care on social media companies to protect children, “needs to work”. Tom Tugendhat, the security minister, said parents should talk to their children about using social media. “The platforms you presume safe may pose a risk,” he said, adding that tech companies should introduce stronger safeguards to prevent abuse. According to research published last week by the communications regulator, Ofcom, a quarter of three- to four-year-olds own a mobile phone and half of under-13s are on social media. The government is preparing to launch a consultation in the coming weeks that will include proposals to ban the sale of smartphones to under-16s and raise the minimum age for social media sites from 13 to as high as 16.",
        "author": "Dan Milmo",
        "published_date": "2024-04-22T23:01:04+00:00"
    },
    {
        "id": "2e3ba5d3-f558-4736-99dc-a064b2cef60b",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/22/eu-threatens-to-ban-tiktok-lite-over-reward-to-watch-feature",
        "title": "EU threatens TikTok Lite with ban over reward-to-watch feature",
        "content": "The EU has said it will ban a new service launched by TikTok in Europe that it believes could be “as addictive as cigarettes” unless the company offers “compelling” fresh evidence that children are safeguarded. If the ban goes ahead, it would be the first time the EU has used sweeping new powers to impose sanctions on social media companies since its landmark Digital Service Act (DSA) came into force last August. The commission gave TikTok until Wednesday to “bring arguments in its defence which the commission will carefully assess” before it takes a final decision on enforcement steps. The digital commissioner, Thierry Breton, said the Chinese-owned video-sharing platform had “failed to prove” that TikTok Lite, which rewards users for watching clips, complied with obligations under the act, describing the service as “toxic”. The commission said the reward feature could be suspended in the bloc if TikTok did not provide a satisfactory response to regulators’ concerns about the impact on users’ mental health. Breton pointed out that the app was launched in France and Spain this month despite the fact there was an ongoing DSA investigation into the company launched in February over other concerns in regard to the safeguarding of children. He said TikTok was used by millions of children in Europe and the commission was “sparing no effort to protect them”. The new watch-and-get-rewarded application offers users prizes such as Amazon vouchers, gift cards via PayPal or TikTok’s Coins currency for points earned through “tasks”, which include watching videos, liking content, following creators or inviting friends to join. Breton told reporters that TikTok Lite “could be as toxic and addictive as cigarettes”. He said that while TikTok’s main app offered users “fun and a sense of connection”, it also “comes with considerable risk for our children: addiction, anxiety, depression, eating disorders, low attention spans.” TikTok was given a 24-hour deadline last week to provide a risk assessment over the new Lite new service amid concerns it could encourage children to become hooked on watching videos. On Monday the commission said it had not received satisfactory answers from TikTok over safeguards against addiction, despite already being under investigation in relation to other concerns about child safeguarding. “While this first case is ongoing, TikTok chose to launch TikTok Lite, which under the laudable promise of letting you watch videos … creates financial incentives for spending more time on your phone,” it said. Breton wrote on X: “Unless TikTok provides compelling proof of safety, which it has failed to do now, we stand ready to trigger DSA interim measures including the suspension of the TikTok Lite ‘reward programme’. A TikTok spokesperson said: “We are disappointed with this decision – the TikTok Lite rewards hub is not available to under-18s and there is a daily limit on video watch tasks. We will continue discussions with the commission.” The ultimatum comes as the future of the viral video platform’s US operation lies in doubt after lawmakers in Washington passed a bill over the weekend that could ban the app if TikTok’s Chinese owner, ByteDance, does not sell its stake in the American business. TikTok said on Monday it would fight any ban or forced sale in the courts. The investigation launched in February under the DSA looking into the safeguarding of children on TikTok includes the issues of age verification, advertising transparency and the risk management of addictive design and harmful content.",
        "author": "Lisa O'Carroll",
        "published_date": "2024-04-22T18:19:08+00:00"
    },
    {
        "id": "ffb510c0-1599-47fd-b5e1-56dda766e424",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/22/elon-musk-hits-back-at-australian-court-order-against-x-images-of-stabbing",
        "title": "Elon Musk hits back at Australian court order against X images of stabbing",
        "content": "Elon Musk has hit back at the Australian internet watchdog’s attempts to force his social media platform X into blocking users from seeing violent footage relating to the Sydney church stabbing. On Monday evening the Australian federal court ordered Elon Musk’s X to hide posts containing videos of a stabbing at a Sydney church last week from users globally, after the eSafety commissioner launched an urgent court case seeking an injunction. Some hours later the American billionaire posted on his personal X account a cartoon showing the platform as a Wizard of Oz-style path to “freedom” and “truth” with a darker, alternative path to “censorship” and “propaganda”. Above the cartoon Musk has written the message: “Don’t take my word for it, just ask the Australian PM!” He also reposted a post on X highlighting a quote by Anthony Albanese on Monday in which the prime minister talked about the fact that “by and large” most social media sites had responded positively to the Australian attempts to block the footage. However, the post added the words “for censorship” to the prime minister’s quote and claimed Albanese had taken time to “advertise for Elon”. Above the post, Musk added the comment: “I’d like to take a moment to thank the PM for informing the public that this platform is the only truthful one.” On Tuesday, Albanese said the government would “do what’s necessary to take on this arrogant billionaire who thinks he’s above the law, but also above common decency”. He said the eSafety Commissioner was doing her job to protect the interests of Australians. “The idea that someone would go to court for the right to put up violent content on a platform shows how out-of-touch Mr Musk is,” Albanese said. “Social media needs to have social responsibility with it. Mr Musk is not showing any.” It followed a successful court bid on late on Monday by the eSafety commissioner to secure the order against X. X, along with Meta, were ordered by the eSafety commissioner, Julie Inman Grant, on Tuesday last week to remove material deemed to depict “gratuitous or offensive violence with a high degree of impact or detail” within 24 hours or potentially face fines. Sign up for Guardian Australia’s free morning and afternoon email newsletters for your daily news roundup The material was footage of the alleged stabbing of bishop Mar Mari Emmanuel last Monday evening while he was giving a livestreamed service at the Assyrian Christ the Good Shepherd church in Wakeley. In a hearing late on Monday afternoon, barrister for eSafety Christopher Tran told Justice Geoffrey Kennett that X had geo-blocked the posts containing the video, meaning Australians could not access them. However, the posts were still accessible globally, and to Australians who used a virtual private network (VPN) connection that made their IP address appear outside Australia. During a hastily arranged hearing, Tran said the “graphic and violent” video remained online on X, formerly known as Twitter. It would cause “irreparable harm” if it continued to circulate, Tran said. “That was a choice, they could have done more.” At the least, X should shield the footage from all users, not just Australians, he submitted. Anticipating an argument about the United States’ right to free speech, Tran said it appeared that right did not extend to depictions of violence. Musk had earlier branded the eSafety commissioner the “Australian censorship commissar” while his company raised free speech and jurisdictional concerns over the takedown order. X also branded the internet cop’s move an “unlawful and dangerous approach”. Marcus Hoyne, appearing for X Corp, urged the court to postpone the matter until he could seek “sensible and proper instructions” from his San Francisco-based client. The eSafety commissioner’s court application was served at the last possible moment, he said. Granting the order would affect international users “in circumstances where it has no impact on Australia,” he said. His appeal failed, however. The judge granted the interim order sought, suppressing the footage to all users on X until at least Wednesday afternoon. The case will return to court on Wednesday for an argument about a permanent suppression.",
        "author": "Martin Farrer",
        "published_date": "2024-04-22T17:58:52+00:00"
    },
    {
        "id": "8b19dab7-5be4-46fc-802e-c20af6764fa3",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/22/elon-musk-postpones-india-trip-tesla-results-narendra-modi",
        "title": "Tesla shares under pressure after carmaker announces price cuts",
        "content": "Shares in Tesla came under pressure on Monday after the electric carmaker announced a round of price cuts ahead of a difficult set of results for the company’s chief executive, Elon Musk. Tesla stock fell as much as 5% in early trading before recovering slightly to a deficit of 3.4% in the wake of the price reductions around the globe, including slashing the cost of three of its leading electric vehicles (EVs) and its self-driving software. Musk revealed at the weekend that he had postponed a trip to India, including a planned meeting with the prime minister, Narendra Modi, because of “very heavy obligations” at the company. The CEO faces a key conference call with the investment community on Tuesday, when Tesla’s latest quarterly figures are expected to reveal its worst performance in seven years. Tesla’s results come amid slowing global demand for EVs and pressure on prices from Chinese rivals. The company has already indicated a poor first quarter in terms of sales, after it revealed this month that deliveries missed market expectations by about 13%. Tesla attempted to boost demand for its EVs late on Friday by cutting the prices of three of its five models in the US, then went on to cut prices around the world over the weekend, including in China, the Middle East, Africa and Europe. It cut the US prices of the Model Y, Tesla’s most popular model and the top-selling EV, and also of the older and more expensive Models X and S. Those cuts reduced the starting price for a Model Y to $42,990 (£34,874), and to $72,990 for a Model S and $77,990 for a Model X. It also slashed the US price of its Full Self-Driving driver assistance software from $12,000 to $8,000. It also emerged on Friday that Tesla was recalling all 3,878 Cybertrucks it has shipped since the vehicle was released in late 2023 because of a faulty accelerator pedal. A filing from the US safety regulator said owners had reported that the pedal pad could come loose and get lodged in the interior trim, causing the vehicle to accelerate unintentionally, increasing the risk of a crash. Dan Ives, the managing director of the US financial services firm Wedbush Securities, said investors’ reaction to the price cuts on Monday showed they were worried “Tesla is panicking”, as well as reflecting concerns that the reductions would hit margins – a measure of profitability. Tesla has already reacted to the slowdown by cutting more than 10% of its global workforce, equivalent to at least 14,000 roles. Musk faces questions on Tuesday about growth in China, plans for a cheaper electric car known as the Model 2 and whether a reported switch in focus to self-driving robotaxis will affect the project. Shares in Tesla have declined more than 40% so far this year. Analysts at Wedbush wrote last week that Tuesday’s conference call represented a “moment of truth” for Musk and Tesla. “While we have seen much more tenuous times in the Tesla story going back to 2015, 2018, 2020 … this time is clearly a bit different as for the first time many longtime Tesla believers are giving up on the story and throwing in the white towel,” Wedbush wrote in a note to investors. Reuters reported this month that Tesla had halted development of the Model 2, prompting Musk to post on X that “Reuters is lying”, without citing any inaccuracies. Musk said this weekend he would reschedule the India trip to a later date this year. He had been due to visit on 21 April and 22 April, where he had been expected to announce an investment of $2bn-3bn in India, according to Reuters, with the spending plans focused on building a new plant. Musk’s now-postponed visit to Delhi had also been expected to include meetings with executives at space industry startups. The billionaire is awaiting Indian government regulatory approvals to begin offering his Starlink satellite broadband service in the country.",
        "author": "Dan Milmo",
        "published_date": "2024-04-22T15:51:22+00:00"
    },
    {
        "id": "dc384208-42a6-4dbb-9a62-e06636f70861",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/22/tiktok-us-ban-or-forced-sale-bill-bytedance",
        "title": "TikTok says it will fight US ban or forced sale after bill passes",
        "content": "TikTok has said it will fight any ban or forced sale of the app’s US operation in the courts, after the House of Representatives passed legislation targeting the viral video platform. The company’s future in the US was placed in further doubt over the weekend after lawmakers in Washington passed a bill that will ban the app if TikTok’s Chinese owner, ByteDance, does not sell its stake in the American business. The House passed the legislation on Saturday by a margin of 360 to 58, as part of a foreign aid package for Ukraine, Israel and Taiwan. The TikTok bill will go to the Senate, where it could be voted through this week. Joe Biden has previously said he would back the legislation. TikTok’s head of public policy for the Americas, Michael Beckerman, told staff in a memo after the vote that the bill was unconstitutional and TikTok would fight it in the courts. “At the stage that the bill is signed, we will move to the courts for a legal challenge,” he wrote in the memo, which was first reported by the tech news website The Information. Beckerman claimed that the bill violated the first amendment of the US constitution, which protects freedom of speech. “We’ll continue to fight, as this legislation is a clear violation of the first amendment rights of the 170 million Americans on TikTok,” he wrote. The first amendment argument has already been deployed to TikTok’s benefit in the US. Last year a district judge in Montana blocked the state’s ban on the use of TikTok, saying it violated the free speech rights of users. Donald Molloy ruled the ban “oversteps state power and infringes on the constitutional rights of users”. TikTok is under pressure from lawmakers in the US, and other western politicians including in the UK, over fears that its data about users can be accessed by the Chinese government. TikTok denies that Beijing authorities have demanded access to user data and says it would refuse if asked to do so. However, TikTok’s critics say ByteDance would be forced under Chinese security laws to share data with security services if asked to do so. TikTok has been contacted for comment.",
        "author": "Dan Milmo",
        "published_date": "2024-04-22T11:02:40+00:00"
    },
    {
        "id": "8d504c96-9928-4901-ba8a-6397b1256477",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/22/lawsuit-in-london-to-allege-grindr-shared-users-hiv-status-with-ad-firms",
        "title": "Lawsuit in London to allege Grindr shared users’ HIV status with ad firms",
        "content": "Grindr faces the prospect of legal action by hundreds of users who will allege that the dating app shared highly sensitive personal information, including in some cases their HIV status, with advertising companies. The law firm Austen Hays is to file a claim on Monday in London’s high court alleging that the US owner of the app breached British data protection laws. The firm alleges that thousands of UK Grindr users had their information misused. So far 670 people have signed up to the claim, and the firm said “thousands” more people had expressed interest in joining. Grindr said it would respond vigorously to the claim, which it said relied on a mischaracterisation of past policies. Grindr was founded in 2009 to make it easier to coordinate hookups for gay men. It now claims to be the world’s largest dating app for gay, bi, trans and queer people, with millions of users around the world. The high court claim against Grindr will focus on the company’s alleged sharing of personal information with two advertising companies. It will further allege that those companies may have sold on the data to other businesses. The law firm said the claim against Grindr will be focused on the periods before 3 April 2018 and between 25 May 2018 and 7 April 2020, meaning newer users are unlikely to be able to join. Grindr changed its consent mechanisms in April 2020. Grindr, based in Los Angeles, announced it would stop sharing users’ HIV status with third-party companies in April 2018 after a report by Norwegian researchers revealed data sharing with two companies. In 2021 Norway’s data protection authority fined Grindr 65m Norwegian krone (£4.8m) – 10% of its global revenues – for violating the General Data Protection Regulation, and the country’s privacy appeals board upheld the decision last year. Grindr appealed against that decision. Norway’s decision did not focus on the alleged sharing of users’ HIV status, but rather found that sharing the fact that someone was signed up to Grindr in itself was sensitive information, as users were very likely to be part of the gay or bi community. Chaya Hanoomanjee, managing director of Austen Hays, who is leading the claim, said: “Our clients have experienced significant distress over their highly sensitive and private information being shared without their consent, and many have suffered feelings of fear, embarrassment and anxiety as a result. “Grindr owes it to the LGBTQ+ community it serves to compensate those whose data has been compromised and have suffered distress as a result, and to ensure all its users are safe while using the app, wherever they are, without fear that their data might be shared with third parties.” The law firm said it believed some users could be entitled to thousands of pounds in damages, without giving further details. A Grindr spokesperson said: “We are committed to protecting our users’ data and complying with all applicable data privacy regulations, including in the UK. “We are proud of our global privacy program and take privacy extremely seriously. We intend to respond vigorously to this claim, which appears to be based on a mischaracterisation of practices from more than four years ago, prior to early 2020.”",
        "author": "Jasper Jolly",
        "published_date": "2024-04-22T06:00:20+00:00"
    },
    {
        "id": "99272cc2-f353-444e-8b55-bc11a57b57d7",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/21/tesla-sales-price-cuts-elon-musk",
        "title": "Tesla cuts prices around the world as sales decline in a chaotic week",
        "content": "Tesla slashed prices of three of its five models in the US late on Friday, then went on to cut prices around the globe – including in China and Germany – as the company faces falling sales, a Cybertruck recall and an intensifying war for electric vehicles (EVs). On Friday, the company, led by the billionaire Elon Musk, cut the prices of the Model Y, a small SUV that is Tesla’s most popular model and the top-selling electric vehicle in the US, and also of the Models X and S, its older and more expensive models. Prices for the Model 3 sedan and the Cybertruck stayed the same. The cuts reduced the starting price for a Model Y to $42,990, and to $72,990 for a Model S and $77,990 for a Model X. Then on Saturday, Tesla slashed the US price of its “Full Self-Driving” driver assistant software from $12,000 to $8,000. The cuts continued on Sunday, when Tesla cut the starting price of the revamped Model 3 in China by 14,000 yuan ($1,930) to 231,900 yuan ($32,000), its official website showed. In Germany, the carmaker trimmed the price of its Model 3 rear-wheel drive to €40,990 ($43,670.75) from €42,990, where the price has been since February. There were also price cuts in many other countries in Europe, the Middle East and Africa, a Tesla spokesperson said. The swathe of price cuts comes after the company reported this month that its global vehicle deliveries in the first quarter had fallen for the first time in nearly four years. Tesla shares fell below $150 this week, wiping away all gains the company made in the past year. The vehicle manufacturer said on Monday that it was cutting 10% of its staff globally, about 14,000 jobs. Musk has also faced criticisms from investors who say he is stretched too thin after his purchase of the social media platform Twitter, which he renamed X, in 2022. Meanwhile, Tesla has asked its shareholders to vote in favor of Musk’s $56bn pay package, which was rejected this year by a judge who called it an “unfathomable sum”. The price cuts ended a chaotic week for Tesla that included a recall of all Cybertrucks on Friday. Federal regulators contacted the company after becoming aware of malfunctions with the vehicle’s accelerator pedal. Cybertruck owners reported that their vehicles were at risk of getting stuck driving at full speed due to the pedal being loose. Video showed the pedal falling off and the piece beneath wedging itself into the car’s interior, which would force the vehicle into maximum acceleration. One driver was able to save himself from a crash by holding down the brake pedal. “This is another black eye for Tesla, which has added to the chaos going on for Musk,” said Dan Ives, senior equity analyst at Wedbush Securities. “Cybertruck is the pedestal moment and a recall out of the gates is a bad look.” Industry analysts have been waiting for Tesla to introduce a small electric vehicle that would cost about $25,000, the Model 2. Media reports this month that Musk planned to scrap the project created more uncertainty over the company’s direction, although Musk called them untrue. On Saturday, Musk confirmed he had postponed a planned weekend trip to India to meet with that country’s prime minister, Narendra Modi, citing “very heavy Tesla obligations”. He said on X that he looked forward to rescheduling the visit for later this year. Tesla is scheduled to announce its first-quarter earnings on Tuesday. The company reported this month that its worldwide sales had fallen sharply from January through March as competition increased worldwide, electric vehicle sales growth slowed and earlier price cuts failed to lure more buyers. It was Tesla’s first year-over-year quarterly sales decline in nearly four years. Kari Paul contributed reporting",
        "author": "",
        "published_date": "2024-04-21T17:46:19+00:00"
    },
    {
        "id": "196970e6-c2f6-4da2-a062-feedf500ff61",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/21/sex-offender-banned-from-using-ai-tools-in-landmark-uk-case",
        "title": "Sex offender banned from using AI tools in landmark UK case",
        "content": "A sex offender convicted of making more than 1,000 indecent images of children has been banned from using any “AI creating tools” for the next five years in the first known case of its kind. Anthony Dover, 48, was ordered by a UK court “not to use, visit or access” artificial intelligence generation tools without the prior permission of police as a condition of a sexual harm prevention order imposed in February. The ban prohibits him from using tools such as text-to-image generators, which can make lifelike pictures based on a written command, and “nudifying” websites used to make explicit “deepfakes”. Dover, who was given a community order and £200 fine, has also been explicitly ordered not to use Stable Diffusion software, which has reportedly been exploited by paedophiles to create hyper-realistic child sexual abuse material, according to records from a sentencing hearing at Poole magistrates court. The case is the latest in a string of prosecutions where AI generation has emerged as an issue and follows months of warnings from charities over the proliferation of AI-generated sexual abuse imagery. Last week, the government announced the creation of a new offence that makes it illegal to make sexually explicit deepfakes of over-18s without consent. Those convicted face prosecution and an unlimited fine. If the image is then shared more widely offenders could be sent to jail. Creating, possessing and sharing artificial child sexual abuse material was already illegal under laws in place since the 1990s, which ban both real and “pseudo” photographs of under-18s. In previous years, the law has been used to prosecute people for offences involving lifelike images such as those made using Photoshop. Recent cases suggest it is increasingly being used to deal with the threat posed by sophisticated artificial content. In one going through the courts in England, a defendant who has indicated a guilty plea to making and distributing indecent “pseudo photographs” of under-18s was bailed with conditions including not accessing a Japanese photo-sharing platform where he is alleged to have sold and distributed artificial abuse imagery, according to court records. In another case, a 17-year-old from Denbighshire, north-east Wales, was convicted in February of making hundreds of indecent “pseudo photographs”, including 93 images and 42 videos of the most extreme category A images. At least six others have appeared in court accused of possessing, making or sharing pseudo-photographs – which covers AI generated images – in the last year. The Internet Watch Foundation (IWF) said the prosecutions were a “landmark” moment that “should sound the alarm that criminals producing AI-generated child sexual abuse images are like one-man factories, capable of churning out some of the most appalling imagery”. Susie Hargreaves, the charity’s chief executive, said that while AI-generated sexual abuse imagery currently made up “a relatively low” proportion of reports, they were seeing a “slow but continual increase” in cases, and that some of the material was “highly realistic”. “We hope the prosecutions send a stark message for those making and distributing this content that it is illegal,” she said. It is not clear exactly how many cases there have been involving AI-generated images because they are not counted separately in official data, and fake images can be difficult to tell from real ones. Last year, a team from the IWF went undercover in a dark web child abuse forum and found 2,562 artificial images that were so realistic they would be treated by law as though they were real. The Lucy Faithfull Foundation (LFF), which runs the confidential Stop It Now helpline for people worried about their thoughts or behaviour, said it had received multiple calls about AI images and that it was a “concerning trend growing at pace”. It is also concerned about the use of “nudifying” tools used to create deepfake images. In one case, the father of a 12-year-old boy said he had found his son using an AI app to make topless pictures of friends. In another case, a caller to the NSPCC’s Childline helpline said a “stranger online” had made “fake nudes” of her. “It looks so real, it’s my face and my room in the background. They must have taken the pictures from my Instagram and edited them,” the 15-year-old said. The charities said that as well as targeting offenders, tech companies needed to stop image generators from producing this content in the first place. “This is not tomorrow’s problem,” said Deborah Denis, chief executive at the LFF. The decision to ban an adult sex offender from using AI generation tools could set a precedent for future monitoring of people convicted of indecent image offences. Sex offenders have long faced restrictions on internet use, such as being banned from browsing in “incognito” mode, accessing encrypted messaging apps or from deleting their internet history. But there are no known cases where restrictions were imposed on use of AI tools. In Dover’s case, it is not clear whether the ban was imposed because his offending involved AI-generated content, or due to concerns about future offending. Such conditions are often requested by prosecutors based on intelligence held by police. By law, they must be specific, proportionate to the threat posed, and “necessary for the purpose of protecting the public”. A Crown Prosecution Service spokesperson said: “Where we perceive there is an ongoing risk to children’s safety, we will ask the court to impose conditions, which may involve prohibiting use of certain technology.” Stability AI, the company behind Stable Diffusion, said the concerns about child abuse material related to an earlier version of the software, which was released to the public by one of its partners. It said that since taking over the exclusive licence in 2022 it had invested in features to prevent misuse including “filters to intercept unsafe prompts and outputs” and that it banned any use of its services for unlawful activity.",
        "author": "Shanti Das",
        "published_date": "2024-04-21T06:00:49+00:00"
    },
    {
        "id": "58255fe4-ba4b-494f-a4fe-946aaa98c38b",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/20/us-house-tiktok-byte-dance",
        "title": "US House passes bill that could lead to total TikTok ban",
        "content": "The House of Representatives voted 360 to 58 on the updated divest-or-ban bill that could lead to the first time ever that the US government has passed a law to shut down an entire social media platform. The Senate is expected to vote on the bill next week and Joe Biden has said he will sign the legislation. “This bill protects Americans and especially America’s children from the malign influence of Chinese propaganda on the app TikTok. This app is a spy balloon in Americans’ phones,” said Texas Republican representative Michael McCaul, author of the bill, Bloomberg reports. The updated TikTok bill comes as part of House Republican speaker Mike Johnson’s foreign aid package for Ukraine, Israel and Taiwan. The passage of the updated version of the bill came after Maria Cantwell, the Senate commerce committee chair, urged the House in March to revise the bill’s details, which now extends TikTok’s parent company ByteDance’s divestment period from six months to a year. In a statement released on Tuesday, Cantwell said: “As I’ve said, extending the divestment period is necessary to ensure there is enough time for a new buyer to get a deal done. I support this updated legislation.” Critics of the popular social media app argue that ByteDance, which is based in China, could collect user data and censor content that is critical of the Chinese government. In March, Avril Haines, the director of national intelligence, warned in a House intelligence committee hearing that China could use TikTok to influence the US’s 2024 presidential elections. Meanwhile, TikTok has repeatedly said that it has not and would not share US user data with the Chinese government. “TikTok is an independent platform, with its own leadership team, including a CEO based in Singapore, a COO based in the US and a global head of trust and safety based in Ireland,” the company said. In response earlier this week to the House’s then upcoming vote, TikTok wrote a post on social media expressing its displeasure at the bill and the US’s ability to “shutter a platform that contributes $24bn to the US economy, annually”. Following the bill’s passage, TikTok said: “It is unfortunate that the House of Representatives is using the cover of important foreign and humanitarian assistance to once again jam through a ban bill that would trample the free speech rights of 170 million Americans,” NPR reports. The president of Signal, an encrypted messaging service and US company, also condemned the bill’s passage, arguing that the data privacy arguments could be extended to other social media companies while pointing to the Senate’s recent passage of the reauthorization of the Foreign Intelligence Surveillance Act that expands warrantless surveillance powers. In a post on X, Meredith Whittaker said: “This is fucked. Please take a moment to consider what’s happening here. Abuse of surveillance powers is about to be enshrined in US law at the same time that a bill to force TikTok to sell to US buyer or be banned is moving forward, justified in part via ‘data privacy.’” In March, Joe Biden vowed to sign the TikTok bill, saying: “If they pass it. I’ll sign it.” That same month, Shou Zi Chew testified before Congress for more than five hours during which lawmakers grilled TikTok’s Singaporean CEO on China, drugs and teenage mental health.",
        "author": "Maya Yang",
        "published_date": "2024-04-20T19:42:08+00:00"
    },
    {
        "id": "d4b93d85-31c8-4d57-9cbd-df7080b0ab94",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/20/cybertruck-failures-tesla-elon-musk",
        "title": "What the Cybertruck’s many failures mean for Tesla",
        "content": "Tesla recalled all Cybertrucks Friday after federal safety regulators contacted the company over malfunctions with the vehicle’s accelerator pedal. New Cybertruck orders have been reportedly cancelled or stalled. The news follows numerous reports of embarrassing Cybertruck failures. The recall represents a major blow to Tesla, which has weathered a difficult year, seeing poor earnings reports in recent quarters as competing Chinese electric vehicle manufacturers encroach on the electric vehicle market. “This is another black eye for Tesla, which has added to the chaos going on for Musk,” said Dan Ives, senior equity analyst at Wedbush Securities. “Cybertruck is the pedestal moment and a recall out of the gates is a bad look.” Cybertruck owners reported that their vehicles were at risk of getting stuck driving at full speed due to a loose accelerator pedal. Video showed the pedal itself falling off and the piece beneath wedging itself into the car’s interior, which would force the vehicle into maximum acceleration. One driver was able to save himself from a crash by holding down the brake pedal. As of Monday, the US National Highway Traffic Safety Administration (NHTSA) had contacted Tesla regarding the issue, and the company announced the recall Friday. Tesla first became aware of this issue on 31 March, according to the filing. After assessing the problem, Tesla on 12 April decided to issue a voluntary recall of the Cybertrucks, the filing says. The Cybertruck, which has long been a pet project for Elon Musk, the Tesla CEO, began deliveries in late 2023 after years of delay due to production problems and battery-supply constraints. Since then, numerous failures in the vehicle’s design and function have ranged from embarrassing to outright dangerous. The trucks – which Musk once claimed would be the “best off-road vehicle” – have been shown getting stuck in sand, snow and dirt, with one towed away by a Ford truck. Some owners have reported their new Cybertrucks have simply stopped running completely. Many have complained the truck’s stainless steel exterior rusts easily, and one owner said the windshield broke quickly in a hail storm. Musk himself claimed the car was bulletproof at its unveiling before cracking its window with a steel ball thrown by hand. In response to the reports of rust, Tesla says in its Cybertruck Do It Yourself guide: “Your Cybertruck is not rusting. These spots are surface contamination caused by iron-containing debris that is picked up by your vehicle as you drive.” Owners can remove them with isopropyl alcohol, according to the guide. Tesla did not respond to requests for comment regarding these reports, or the recall. In response to request for comment, the US NHTSA shared Tesla’s recall filing. Recalls are not uncommon in the auto industry and it is unclear how severe the accelerator pedal issue is, said Thomas Monteiro, senior analyst at Investing.com. But amid the ongoing uncertainty at Tesla, the company had no choice but to take action, he said. “At this point, the company simply cannot risk the potential liability or bad publicity,” Monteiro said. Tesla shares fell below $150 this week, wiping away all gains the company made in the past year. The vehicle manufacturer said Monday that it was cutting 10% of its staff globally, about 14,000 jobs. Musk has also faced criticisms from investors who say he is stretched too thin after his purchase of social media platform Twitter, which he renamed X, in 2022. Meanwhile, Tesla has asked its shareholders to vote in favor of Musk’s $56bn pay package that was rejected by a judge earlier this year who called it an “unfathomable sum”. “Against this backdrop, Musk needs to keep investors convinced that the company’s projects are pursuing the right path and that these will lead to margin appreciation over time,” he said. “An epic failure on the Cybertruck could very well jeopardize that perception, especially given that the company hasn’t really launched any groundbreaking innovations in years now.” The speed with which Tesla recalled the vehicles after reports of the faulty pedal may be influenced by its continuing legal battles over injuries caused by its semi-autonomous driving software. The company is facing a number of individual and class-action lawsuits over allegations the technology caused fatal accidents. Brett Schreiber, an attorney who has represented clients affected by the faulty software, said he anticipates another potential wave of litigation from Cybertruck owners. “What we have seen perpetually with Tesla is the ethos of a tech company, wanting to push out product as quickly as possible, wrapped up in an automotive manufacturer, which should be far more intentional and thoughtful in producing vehicles,” he said. “This is not an app, this is a multithousand-pound vehicle hurtling down our roadways at high speeds.” Tesla is set to report its first-quarter earnings on 23 April, just a few days away, when investors will likely be keen to hear updates on the recall.",
        "author": "Kari Paul",
        "published_date": "2024-04-20T13:00:28+00:00"
    },
    {
        "id": "fe7c471b-6898-4ca0-9225-51c19afbf2d7",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/19/oxford-future-of-humanity-institute-closes",
        "title": "Oxford shuts down institute run by Elon Musk-backed philosopher",
        "content": "Oxford University this week shut down an academic institute run by one of Elon Musk’s favorite philosophers. The Future of Humanity Institute, dedicated to the long-termism movement and other Silicon Valley-endorsed ideas such as effective altruism, closed this week after 19 years of operation. Musk had donated £1m to the FHI in 2015 through a sister organization to research the threat of artificial intelligence. He had also boosted the ideas of its leader for nearly a decade on X, formerly Twitter. The center was run by Nick Bostrom, a Swedish-born philosopher whose writings about the long-term threat of AI replacing humanity turned him into a celebrity figure among the tech elite and routinely landed him on lists of top global thinkers. Sam Altman of OpenAI, Bill Gates of Microsoft and Musk all wrote blurbs for his 2014 bestselling book Superintelligence. “Worth reading Superintelligence by Bostrom. We need to be super careful with AI. Potentially more dangerous than nukes,” Musk tweeted in 2014. Bostrom resigned from Oxford following the institute’s closure, he said. The closure of Bostrom’s center is a further blow to the effective altruism and long-termism movements that the philosopher had spent decades championing, and which in recent years have become mired in scandals related to racism, sexual harassment and financial fraud. Bostrom himself issued an apology last year after a decades-old email surfaced in which he claimed “Blacks are more stupid than whites” and used the N-word. Bostrom – who popularized the theory that humanity may be living in a simulation, one that Musk often repeats – spoke about the closure of the institute in a lengthy final report published on its website this week. He praised the work of the center, while also saying that it faced “administrative headwinds” from Oxford and its philosophy department. “The closure is the culmination of process that’s been playing out over several years,” Bostrom said via email. “We were funded initially for three years, back in 2005, and then that got extended a number of times. “Eventually a pressure to conform began bearing down (we were administratively housed within the faculty of philosophy, even though the majority of our research team by this time were non-philosophers), and there was a death by bureaucracy.” Bostrom added that he was touched by the number of people speaking out in support of the institute’s work, and that it had been a privilege to work with his colleagues. “FHI was a special place with a unique and highly fruitful intellectual culture,” Bostrom said. “I think we had a good run!” A statement on the Future of Humanity’s website claimed that Oxford had frozen fundraising and hiring in 2020, and that in late 2023 the faculty of philosophy decided to not renew the contracts of remaining staff at the institute. An Oxford University spokesman said: “We regularly consider the best structures for conducting our academic research, as part of the university’s governance processes. After such consideration, the decision was made to close the Future of Humanity Institute. The university recognises the Institute’s important contribution to this emerging field, which researchers elsewhere across the university are likely to continue.” Effective altruism, the utilitarian belief that people should focus their lives and resources on maximizing the amount of global good they can do, has become a heavily promoted philosophy in recent years. The philosophers at the center of it, such as Oxford professor William MacAskill, also became the subject of immense amounts of news coverage and glossy magazine profiles. One of the movement’s biggest backers was Sam Bankman-Fried, the now-disgraced former billionaire who founded the FTX cryptocurrency exchange. Bostrom is a proponent of the related long-termism movement, which held that humanity should concern itself mostly with long-term existential threats to its existence such as AI and space travel. Critics of long-termism tend to argue that the movement applies an extreme calculus to the world that disregards tangible current problems, such as climate change and poverty, and veers into authoritarian ideas. In one paper, Bostrom proposed the concept of a universally worn “freedom tag” that would constantly surveil individuals using AI and relate any suspicious activity to a police force that could arrest them for threatening humanity. Bostrom and long-termism gained numerous powerful supporters over the years, including Musk and other tech billionaires. Bostrom’s Institute received £13.3m in 2018 from the Open Philanthropy Project, a non-profit financially backed by Facebook co-founder Dustin Moskovitz. The past few years have been tumultuous for effective altruism, however, as Bankman-Fried’s multibillion-dollar fraud marred the movement and spurred accusations that its leaders ignored warnings about his conduct. Concerns over effective altruism being used to whitewash the reputation of Bankman-Fried, and questions over what good effective altruist organizations are actually doing, proliferated in the years since his downfall. Meanwhile, Bostrom’s email from the 1990s resurfaced last year and resulted in him issuing a statement repudiating his racist remarks and clarifying his views on subjects such as eugenics. Some of his answers – “Do I support eugenics? No, not as the term is commonly understood” – led to further criticism from fellow academics that he was being evasive. The university launched an investigation into Bostrom’s conduct following the discovery of his racist email, while other major effective altruism groups distanced themselves from him. “We unequivocally condemn Nick Bostrom’s recklessly flawed and reprehensible words,” the Centre for Effective Altruism, which was founded by fellow Oxford philosophers and financially backed by Bankman-Fried, said in a statement at the time. • This article was amended on 21 April 2024 to add a comment from Oxford University.",
        "author": "Nick Robins-Early",
        "published_date": "2024-04-19T22:46:09+00:00"
    },
    {
        "id": "ccecf851-177b-456b-a72d-5ee0a9c74129",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/19/idf-facial-recognition-surveillance-palestinians",
        "title": "How Israel uses facial-recognition systems in Gaza and beyond",
        "content": "Governments around the world have increasingly turned to facial-recognition systems in recent years to target suspected criminals and crack down on dissent. The recent boom in artificial intelligence has accelerated the technology’s capabilities and proliferation, much to the concern of human rights groups and privacy advocates who see it as a tool with immense potential for harm. Few countries have experimented with the technology as extensively as Israel, which the New York Times recently reported has developed new facial-recognition systems and expanded its surveillance of Palestinians since the start of the Gaza war. Israeli authorities deploy the system at checkpoints in Gaza, scanning the faces of Palestinians passing through and detaining anyone with suspected ties to Hamas. The technology has also falsely tagged civilians as militants, one Israeli officer told the Times. The country’s use of facial recognition is one of the new ways that artificial intelligence is being deployed in conflict, with rights groups warning this marks an escalation in Israel’s already pervasive targeting of Palestinians via technology. In an Amnesty International report on Israel’s use of facial recognition last year, the rights group documented security forces’ extensive gathering of Palestinian biometric data without their consent. Israeli authorities have used facial recognition to build a huge database of Palestinians that is then used to restrict freedom of movement and carry out mass surveillance, according to the report. The Israeli ministry of defense did not return a request for comment on the findings of Amnesty’s report or the New York Times article on its facial-recognition programs. The Guardian spoke with Matt Mahmoudi, an adviser on AI and human rights at Amnesty and lead researcher on the report, about how Israel deploys facial-recognition systems and how their use has expanded during the war in Gaza. One thing that stands out from your report is that there’s not just one system of facial recognition but several different apps and tools. What are the ways that Israeli authorities collect facial data? There’s a slew of facial-recognition tools that the state of Israel has experimented with in the occupied Palestinian territories for the better part of the last decade. We’re looking at tools by the names of Red Wolf, Blue Wolf and Wolf Pack. These are systems that have been tested in the West Bank, and in particular in Hebron. All of these systems are effectively facial-recognition tools. In Hebron, for a long time, there was a reliance on something known as the Wolf Pack system – a database of information pertaining to just Palestinians. They would effectively hold a detained person in front of the CCTV camera, and then the operations room would pull information from the Wolf Pack system. That’s an old system that requires this link-up between the operations room and the soldier on the ground, and it’s since been upgraded. One of the first upgrades that we saw was the Blue Wolf system, which was first reported on by Elizabeth Dwoskin in the Washington Post back in 2021. That system is effectively an attempt at collecting as many faces of Palestinians as possible, in a way that’s akin to a game. The idea is that the system would eventually learn the faces of Palestinians, and soldiers would only have to whip out the Blue Wolf app, hold it in front of someone’s face, and it would pull all the information that existed on them. You mentioned that there’s a gamification of that system. How do incentives work for soldiers to collect as much biometric data as possible? There’s a leaderboard on the Blue Wolf app, which effectively tracks the military units that are using the tool and capturing the faces of Palestinians. It gives you a weekly score based on the most amount of pictures taken. Military units that captured the most faces of Palestinians on a weekly basis would be provided rewards such as paid time away. So you’re constantly put into the terrain of no longer treating Palestinians as individual human beings with human dignity. You’re operating by a gamified logic, in which you will do everything in your power to map as many Palestinian faces as possible. And you said there are other systems as well? The latest we’ve seen happen in Hebron has been the additional introduction of the Red Wolf system, which is deployed at checkpoints and interfaces with the other systems. The way that it works is that individuals passing through checkpoints are held within the turnstile, cameras scan their faces and a picture is put up on the screen. The soldier operating it will be given a light indicator – green, yellow, red. If it’s red, the Palestinian individual is not able to cross. The system is based only on images of Palestinians, and I can’t stress enough that these checkpoints are intended for Palestinian residents only. That they have to use these checkpoints in the first place in order to be able to access very basic rights, and are now subject to these arbitrary restrictions by way of an algorithm, is deeply problematic. What’s been particularly chilling about the system has been hearing the stories about individuals who haven’t been able to even come back into their own communities as a result of not being recognized by the algorithm. Also hearing soldiers speaking about the fact that now they were doubting whether they should let a person that they know very well pass through a checkpoint, because the computer was telling them not to. They were finding that increasingly they had a tendency of thinking of Palestinians as numbers that had either green, yellow or red lights associated with them on a computer screen. These facial-recognition systems operate in a very opaque way and it’s often hard to know why they are making certain judgments. I was wondering how that affects the way Israeli authorities who are using them make decisions. All the research that we have on human-computer interaction to date suggests that people are more likely to defer agency to an algorithmic indicator in especially pressing circumstances. What we’ve seen in testimonies that we reviewed has been that soldiers time and time again defer to the system rather than to their own judgment. That simply comes down to a fear of being wrong about particular individuals. What if their judgment is not correct, irrespective of the fact that they might know the person in front of them? What if the computer knows more than they do? Even if you know that these systems are incredibly inaccurate, the fact that your livelihood might depend on following a strict algorithmic prescription means that you’re more likely to follow it. It has tremendously problematic outcomes and means that there is a void in terms of accountability. What has the Israeli government or military said publicly about the use of these technologies? The only public acknowledgment that the Israeli ministry of defense has made – as far as the Red Wolf, Blue Wolf and Wolf Pack systems – is simply saying that they of course have to take measures to guarantee security. They say some of these measures include innovative tech solutions, but they’re not at liberty to discuss the particularities. That’s kind of the messaging that comes again and again whenever they’re hit with a report that specifically takes to task their systems for human rights violations. What’s also interesting is the way in which tech companies, together with various parts of the ministry of defense, have boasted about their technological prowess when it comes to AI systems. I don’t think it’s a secret that Israeli authorities rely on a heavy dose of PR that touts their military capabilities in AI as being quite sophisticated, while also not being super detailed about exactly how these systems function. There’s been past statements from the Israeli government, I’m thinking specifically of a statement on the use of autonomous weapons, that argue these are sophisticated tools that could actually be good for human rights. That they could remove the potential for harm or accidents. What is your take on that argument? If anything, we have seen time and time again how semi-autonomous weapons systems end up dehumanizing people and leading to deaths that are later on described as “regrettable”. They don’t have meaningful control and they effectively turn people into numbers crunched by an algorithm, as opposed to a moral, ethical and legal judgment that has been made by an individual. It has this sort of consequence of saying: “Well, look, identifying what counts as a military target is not up to us. It’s up to the algorithm.” Since your report came out, the New York Times reported that similar facial-recognition tech has been developed and deployed to surveil Palestinians in Gaza. What have you been seeing in terms of the expansion of this technology? We know that facial-recognition systems were being used for visitors coming in and out of Gaza, but following the New York Times reporting it’s the first time that we’ve heard it being used in the way that it has been – particularly against Palestinians in Gaza who are fleeing from the north to the south. What I’ve been able to observe is largely based on open-source intelligence, but what we see is footage that shows what look like cameras mounted on tripods that are situated outside of makeshift checkpoints. People move slowly from north to south through these checkpoints, and then people are pulled out of the crowd and detained on the basis of what we suspect is a facial-recognition tool. The issue there is that people are already being moved around chaotically from A to B under the auspices of being brought to further safety, only to be slowed down and asked to effectively be biometrically checked before they’re allowed to do so. We also hear about individuals that have been detained and beaten and questioned after having been biometrically identified, but later have it been established that was a mistake. The Israeli government’s justifications for its use of checkpoints and detentions is that it’s based on national security fears. When it comes to facial recognition, how does that argument hold up from a human rights perspective? Your rights aren’t less viable or active just because there’s a national security threat. There is, however, a three-part test that you deploy when it comes to figuring out in what particularly unique kinds of circumstances a state would violate certain rights in order to uphold others. That three-part test tries to assess the necessity, proportionality and legitimacy of a particular intervention. At Amnesty, under international human rights law, we don’t believe that there is necessity, proportionality or legitimacy. This facial-recognition technology isn’t compatible with the right to privacy, the right to non-discrimination, the right to peaceful assembly or freedom of movement. All these rights are severely compromised under a system that is, by design, effectively a system of mass surveillance and therefore unlawful. You’ve talked about the lack of accountability and accuracy with these tools. If the Israeli government knows these are not accurate, then why continue to use them? I think AI-washing is a significant part of how governments posture that they’re doing something about a problem that they want to seem like they’re being proactive on. It’s been clear since the 2010s that governments around the globe have relied on tech solutions. Now, since the explosion of generative AI in particular, there’s this idea that AI is going to solve some of the most complex social, economic and political issues. I think all it does is absolve states of the responsibilities that they have to their citizens – the obligations that they have under international law to uphold the rights of those whom they subject to their power by basically saying “the system will take care of it” or “the system was at fault”. It creates these neat grounds for states to be able to seem like they’re doing something, without being held to account on whatever they’re actually doing. There’s a technical system that is mediating both accountability and responsibility. This interview has been edited and condensed for clarity",
        "author": "Nick Robins-Early",
        "published_date": "2024-04-19T17:16:47+00:00"
    },
    {
        "id": "e2bb2385-9f12-4a2b-9552-027b71c51fbe",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/19/amazon-uk-recognise-union-gmb-ballot-coventry-warehouse",
        "title": "Amazon UK could be forced to recognise union as GMB wins right to hold ballot",
        "content": "Amazon could be forced to recognise a trade union for the first time in the UK after members of the GMB at the internet retailer’s Coventry warehouse were granted permission to hold a legally binding ballot. The Central Arbitration Committee (CAC), the independent statutory body that adjudicates on collective bargaining rights, has ruled that a vote should be held at the site to test support for union recognition. A win would give the GMB the right to discuss terms and conditions such as pay, hours and holidays with Amazon. The union has held a series of strikes at the Coventry unit since January last year, demanding a pay rise to £15 an hour and a seat round the table in negotiations with management. Amanda Gearing, a GMB senior organiser, said: “From day one of GMB’s fight for union rights at Amazon it has been a modern-day David and Goliath battle. One year on this is a truly historic moment as workers stand up against the company’s relentless anti-union propaganda.” The union withdrew an application for statutory union recognition to the CAC last year, accusing Amazon of drafting in more than 1,000 extra staff members to skew the decision – something the retailer has denied. After a concerted recruitment campaign at the vast warehouse, the GMB made a second application last month, believing it had signed up more than half the staff. In a decision statement issued on Thursday, the CAC said it had not found that 50% of workers were members – which could have prompted it to impose recognition immediately – but accepted the GMB’s bid for a ballot to be held. The detail of the watchdog’s statement underlined the tensions between union and employer, with Amazon accusing the GMB of making “extremely serious and untrue accusations”, with the “ulterior motive” of trying to influence the CAC panel. The CAC will appoint an independent person to carry out the vote, which would then be expected to happen within 20 working days. To secure recognition, the GMB will need to win a majority of support in the ballot. The “yes” voters must also represent at least 40% of the workers on site. The CAC’s case manager, who took evidence from the union and Amazon, found that the GMB currently represents 35.62% of the 3,088 workers in the proposed bargaining unit, suggesting the union has a fight on its hands. Amazon said its employees have always had the choice of whether or not to join a union. A spokesperson said: “We regularly review our pay to ensure we offer competitive wages and benefits. Our minimum starting pay has increased to £12.30 and £13 an hour depending on location, that’s a 20% increase over two years and 50% since 2018. We also work hard to provide great benefits, a positive work environment and excellent career opportunities.”",
        "author": "Heather Stewart",
        "published_date": "2024-04-19T10:17:29+00:00"
    },
    {
        "id": "34408d9d-9057-4a07-a332-813c2d50df99",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/apr/19/what-is-bitcoin-halving-price",
        "title": "What is bitcoin halving – and will it affect the price?",
        "content": "Satoshi Nakamoto, the pseudonymous creator of bitcoin, still has an influence on the cryptocurrency nearly 14 years after disappearing. This week the protocol designed by Nakamoto – an individual or group of individuals who went silent in December 2010 – will trigger what is known as a “bitcoin halving”, a process that has coincided with price increases in the past. The latest halving is expected to take place on Saturday. Here we explain what the bitcoin halving entails and its potential impact. What is bitcoin halving? It is related to how bitcoins are recorded and created. Transactions in the cryptocurrency are recorded on a universally accessible ledger called a blockchain. These transactions are put on the blockchain by “miners” who pack them into blocks that are then linked – or “chained” – together. They do this by solving a cryptographic puzzle using specialised hardware and – this is the key bit – receive a reward in newly created bitcoins. Nakamoto intended the number of bitcoins entering circulation to be finite, at 21m, so the protocol seeks to control the amount of new coins entering the market. It does this by halving the size of the miners’ reward every 210,000 blocks – roughly every four years. The latest halving is expected to take place in the early hours of Saturday in the US and UK, when the reward for adding a new block of transactions to the blockchain will decrease from 6.25 bitcoins to 3.125. Bitcoin – of which there are more than 19m in circulation – will continue to halve until the 21m point is reached, expected in 2140. What will the impact be on the bitcoin price? Halving reduces the supply of new bitcoins, which should in theory increase the price. It is an economic axiom that if demand for an asset remains stable while its supply decreases, its price should go up. The past three halvings – in 2020, 2016 and 2012 – have resulted in an average price increase of 16% over the 60 days that followed, according to data from the asset research firm 10x Research. The 2016 halving resulted in a decrease of 6% over the following 60 days, although it then rallied strongly throughout 2017. Markus Thielen, the head of research at 10x, says the halving is “associated with price increases due to reduced supply” but investors will have to wait for a price peak, which typically comes 500 days after a halving. In recent weeks bitcoin has fallen sharply from a recent record high of more than $70,000 (£56,175) to about $62,000 but it remains a strongly performing asset, up 40% so far in 2024 and more than double where it was at the same time last year. It is worth noting that while prices ultimately rose after the 2016 and 2020 halvings, they underwent prolonged dips – so-called “crypto winters” in 2018 and 2022 where prices underwent a prolonged dip. “The setup feels really familiar to past occasions where there has been a very sharp rally and it forms a top … then breaks,” says Neil Wilson, the chief analyst at the brokerage firm Finalto. Analysts at Deutsche Bank wrote on Thursday that the halving was “already partially priced in by the market” and that they did “not expect prices to increase significantly following the halving event”. Will there be a negative impact? Bitcoin mining companies, which take on the energy and equipment costs of validating transactions, face a financial hit as their reward drops. Andrew O’Neill, the managing director of the digital assets research lab at the credit ratings firm S&amp;P Global, wrote this week: “The block reward remains a significant part of miners’ revenue, therefore halving the reward impacts profitability.” He added: “Some operations will become non-profitable and will shut down as result, particularly those with higher energy costs.” To make bitcoin mining financially sustainable, S&amp;P says, the currency will need to be used more widely throughout the global economy in order to increase miners’ revenues via transaction fees. However, greater use of the cryptocurrency jars with concerns that energy-intensive bitcoin mining is already environmentally unsustainable. There is also, for bitcoin’s many critics, the negative impact of amateur investors being lured in by any price rise – and hype – that follows the halving. Bitcoin has gained in legitimacy this year, increasing its price, with US Securities and Exchange Commission permitting exchange-traded funds (ETFs) – a basket of assets that can be bought and sold like shares on an exchange – that track the price of the cryptocurrency. Nonetheless, the chair of the SEC, Gary Gensler, was begrudging in giving the go-ahead, describing bitcoin as a volatile asset used for “illicit activity including ransomware, money laundering, sanction evasion, and terrorist financing”. O’Neill is also sceptical that there will be a price boom. “The BTC [bitcoin] market is in a very different place to when the prior halvings occurred four, eight and 12 years ago,” he says. “Other drivers such as the growth of BTC ETFs in the US, and macro drivers such as interest rates and market liquidity, will also influence price.” Carol Alexander, a professor of finance at the University of Sussex business school, says any price boost from the halving will ultimately be illusory. “It will probably go above the all-time high but in the long run its value will be zero because there is no intrinsic value in bitcoin whatsoever,” she says. “It’s simply a speculative asset.”",
        "author": "Dan Milmo",
        "published_date": "2024-04-19T06:00:05+00:00"
    }
]