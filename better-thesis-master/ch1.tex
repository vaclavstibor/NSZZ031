\chapter{Theoretical Background}
\label{chap:theoretical-background}
Since the application's core is sentiment analysis, it is necessary to define the basic concepts. This chapter provides an overview of sentiment analysis, focusing on the levels of sentiment analysis and named entity recognition. We emphasised that sentiment analysis is most suitable for our application at the entity level, especially in the context of financial texts. Since sentiment analysis in different domains and languages faces many challenges, we decided to focus exclusively on the financial domain in English, allowing us to achieve higher accuracy and specificity in our analyses. 

This chapter also introduced the techniques used in named entity recognition, which are crucial for entity-level sentiment analysis. These techniques include knowledge-based approaches, attribute engineering, and deep learning, proving particularly effective in capturing contextual information in text and achieving state-of-the-art results in entity recognition.

\section{Sentiment Analysis Basics}
\label{sec:sentiment-analysis-basics}
\acrfull{sa} or opinion mining is a subfield of \acrfull{nlp} that aims to identify and extract opinions and emotions from a text. The~goal is to determine the author's attitude towards a particular topic or the overall contextual polarity~of various document levels. We measure the text's polarity using a numerical scale ranging from $-1$ to $1$. The low-end score of the scale signifies a negative sentiment, zero represents neutrality, and the high-end score indicates a positive sentiment. This scale effectively estimates the degree of negativity or positivity in the text's tone. 

The extraction of opinions and emotions has applications in various areas, from product reviews to political events. Hence, it is imperative to work in different domains (see \cite{PIRYANI2017122}). Because cross-domain and cross-language are two of the most general issues in sentiment analysis, this thesis will focus only on the financial domain in English. Nevertheless, domain-specific sentiment analysis achieves remarkable accuracy while staying highly domain-sensitive, as shown in \parencite{saunders_2020}. To delve deeper into cross issues, \citeauthor{liu2022sentiment} provides further details in his book \parencite{liu2022sentiment}.

\subsection{Levels of Sentiment Analysis}
\label{subsec:levels-of-sentiment-analysis}
Sentiment analysis has been studied at several levels of granularity: Document-level, Sentence-level, Phrase-level, and Entity-level\footnote{Entities are sometimes referred to as targets, hence Target-level or Target-based sentiment analysis.}, as illustrated in Figure \ref{fig:sa-levels}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{img/theoretical/sa-levels.pdf}
    \caption{Levels of sentiment analysis (inspired by \cite{Wankhade2022}).}
    \label{fig:sa-levels}
\end{figure}

\subsubsection*{Document-level}
\label{subsubsec:document-level}
Document-level sentiment analysis is the most straight level. The task is to determine the overall emotional context of the entire document, such as a chapter, article, or review, whether or not involving a study of entities or aspects. This level gives us a general assessment of whether the content is more likely to be positive, negative, or neutral. 

\subsubsection*{Sentence-level}
\label{subsubsec:sentence-level}
Sentiment analysis at the sentence level focuses on individual sentences within the text. We observe the polarity of each sentence autonomously, employing the same methodologies utilised at the document level but with an increased volume of training data and enhanced processing resources. This level is more challenging than the document level because it requires a more in-depth understanding of the text. 

\subsubsection*{Phrase-level}
\label{subsubsec:phrase-level}
Phrase-level sentiment analysis examines sentiment within smaller linguistic units such as phrases or sentence members. Thus, it can better reveal the emotional charge in specific parts of sentences. Additionally, this level is more challenging than the sentence level because it requires a more detailed understanding of the text. 

\subsubsection*{Entity-level}
\label{subsubsec:entity-level}
The most elaborative level is entity-level sentiment analysis, where we study sentiment associated with specific entities mentioned in the text. This level provides a detailed look at the expressed polarity of certain products, individuals, or organisations. One of the main tasks in this scope is the named entity recognition, which will be discussed later.

\paragraph{}

Some researchers classify the last level as the aspect-level, as noted by \cite{Wankhade2022}, or a more detailed entity-level version called the feature-level proposed by \cite{Jenifer2017}. While both approaches aim to evaluate sentiment towards specific aspects, they differ in their task approach. Relationships between these levels are illustrated in Figure \ref{fig:entity-feature-aspect-level}.

In the first case, aspects are considered without directly mentioning entities in the text. We are not interested in the entities since the input textual data are commonly associated with them\footnote{Entities are not handled in this case, but we provide them here for a better understanding.}, such as reviews. The study conducted by \cites{Wang2019} analysed sentiment at the aspect level within restaurant reviews. It primarily examines aspects such as food, price, service, and others. In~the feature-based approach, aspects are commonly associated with an entity's features by connecting the entity and its aspects in text. To illustrate, consider the sentence:\begin{quote}
    \textit{``The battery life of this phone is excellent, but the camera is not good.''}
\end{quote} At the feature level, we identify \textit{the battery life} and \textit{camera} as specific features of entity \textit{the phone}, allowing us to determine the polarity of each entity's feature.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{img/theoretical/entity-feature-aspect-level.pdf}
    \caption{Comprehensive overview of the last level.}
    \label{fig:entity-feature-aspect-level}
\end{figure}

The term entity-level sentiment analysis is frequently employed in literature, and some studies consider it synonymous with targeted sentiment analysis, \linebreak as discussed \cite{ronningstad-etal-2022-entity} in the terminology review. For our purposes, entity-level sentiment analysis better captures the aggregate, document-wide approach, where a single entity can be associated with multiple targets in different sentences, discerning it from traditional target-level sentiment analysis. 

However, this thesis primarily focuses on entity-level sentiment analysis, excluding consideration of the entity's features. This decision is motivated by treating the mentioned companies in news articles as entities rather than delving into their specific aspects. Additionally, entity and aspect extraction as separate tasks are complex and challenging, given that the methods employed for recognition differ due to their distinct characteristics \parencite{Liu2015, Zhang2014}. 

\section{Named Entity Recognition}
\label{sec:named-entity-recognition}
\acrfull{ner}, or entity extraction, constitutes a fundamental component in \acrshort{nlp} dedicated to identifying and classifying proper nouns into predefined semantic classes. These classes are unlimited since entities could be anything we can categorise using a tag, including the names of organisations, people, places, or other available information from unstructured textual data such as time, quantity, and money expressions. Someone well-versed in data analysis must have contemplated the possibility that some sources provide data in which the names of organisations are directly associated with their unique tags in text, but this is not our case (for more details, see Chapter \ref{chap:textual-data}). Understanding the importance of this task, we recognise its essential role in entity-level sentiment analysis, as it allows us to identify the emotion associated with specific entities mentioned in the text.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/theoretical/ner-theoretical-example.pdf}
    \caption{Named entities in the text, along with their associated label classes.}
    \label{fig:named-entity-recognition-theoretical-example}
\end{figure}

The example above illustrates that a single entity can contain more than one word. This challenge is addressed by token tagging formats outlined in the paper \parencite{ramshaw-marcus-1995-text}. Individual words are referred to as tokens. The formats describe the position of each token in a named entity, such as the \acrfull{bio} format, also known as the \acrfull{iob} format as well as other derived names like \acrfull{io} and \acrfull{bmewo}. These formats discharge us from the constraints of entities, which are hard-coded or unreliably specified by regular expressions.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/theoretical/ner-theoretical-example-tags.pdf}
    \caption{Token tagging \acrshort{bio}2 format within the named entities.}
    \label{fig:named-entity-recognition-theoretical-example-tags}
\end{figure}

The tags that assign tokens to each entity class contain prefixes. The prefix ``\textit{I-}'' indicates that the token is contained in the named entity, whereas ``\textit{O-}'' indicates that the token is not contained in any named entity. The prefix ``\textit{B-}'' is slightly more specific and indicates that the token is contained at the beginning of the named entity, followed immediately by a token not containing the ``\textit{O-}'' prefix. A particular case of the \acrshort{bio} approach is the \acrshort{bio}2 format, denoting all tokens beginning with the prefix ``\textit{B-}'', regardless of whether a token with an ``\textit{O-}'' prefix follows. A slightly more detailed approach is \acrshort{bmewo}, where the prefixes ``\textit{B-}'' and ``\textit{O-}'', as in the previous ones, indicate the beginning and absence of the named entity occurrence, respectively. ``\textit{M-}'' prefix symbolises the middle token between ``\textit{B-}'' and ``\textit{E-}'', where the token with the prefix ``\textit{E-}'' ends the named entity. Furthermore, the prefix ``\textit{W-}'' indicates a single-token named entity. Table \ref{table:ner-theoretical-example-tags} below demonstrates how the mentioned sentence could be labeled with \acrshort{io}, \acrshort{bio}, \acrshort{bio}2, and \acrshort{bmewo} formats.

\begin{table}[htbp]
    \centering
    \caption{Token tagging formats \acrshort{io}, \acrshort{bio}, \acrshort{bio}2, \acrshort{bmewo} within the named entities.}
    \label{table:ner-theoretical-example-tags}
    \begin{tabular}[t]{lllll}
        \hline
        &\multicolumn{4}{c}{Format}\\
        \cline{2-5}
 Token&IO&BIO&BIO2&BMEWO\\
        \hline
 Tim&I-PERSON&I-PERSON&B-PERSON&B-PERSON\\
 Cook&I-PERSON&I-PERSON&I-PERSON&E-PERSON\\
 was&O&O&O&O\\
        \dots&O&O&O&O\\
 Apple&I-ORG&I-ORG&B-ORG&B-ORG\\
 Inc&I-ORG&I-ORG&I-ORG&M-ORG\\
 .&I-ORG&I-ORG&I-ORG&E-ORG\\
 on&O&O&O&O\\
 August&I-DATE&I-DATE&B-DATE&B-DATE\\
 24&I-DATE&I-DATE&I-DATE&M-DATE\\
 ,&I-DATE&I-DATE&I-DATE&M-DATE\\
 2011&I-DATE&I-DATE&I-DATE&E-DATE\\
 .&O&O&O&O\\
        \hline
    \end{tabular}
\end{table}

%In a given text, there could be several mentions of separate entities, each possibly referred to directly and indirectly multiple times, with varying polarities. 
\subsection{Techniques}
\label{subsec:Techniques}
Approaches for entity extraction from unstructured data encompass a broad range of techniques, though they commonly converge into three principal categories. Specifically, the work \parencite{keraghel2024survey} classifies them in the following way: Knowledge-based, Feature engineering, and Deep learning, as illustrated in Figure \ref{fig:named-entity-recognition-methods} below.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/theoretical/ner-methods.pdf}
    \caption{Named entity recognition techniques overview.}
    \label{fig:named-entity-recognition-methods}
\end{figure}

\subsubsection*{Knowledge-based}
\label{subsubsec:knowledge-based}
Knowledge-based approaches rely on predefined rules and dictionaries to identify entities. These rules and dictionaries, typically created by domain experts, recognise entities based on their characteristics. Rules can be given as regular expressions, denoting patterns to match character combinations in strings. Established on the chosen method, we separate this category into Rule-based and Dictionary-based or combine these approaches. The main advantage of knowledge-based approaches stems from their interpretability, as the rules and dictionaries can be easily understood and modified. Nevertheless, the principal disadvantage is frequent limitations to the entities in the manually created dictionaries and rules, as a result of which new entities cannot be recognised.

\subsubsection*{Feature engineering}
\label{subsubsec:feature-engineering}
Instead of manually creating a set of rules and a dictionary, feature engineering-based approaches, popularly identified as machine learning, use linguistic and statistical features to identify entities. These features are generally derived from the text and subsequently are used to train a machine learning model to recognise entities. The primary advantage of this approach is the ability to learn more about the data and discover patterns that may not be apparent at first. Additionally, their ability to recognise new entities differs from dictionaries in some cases. However, the main disadvantage is that these approaches require a large amount of labeled training data to be accurate.

Before will continue in describing another methods, we should clarify the data under discussion and the definition of labels. In the early introduction regarding named entity recognition, we referred to tags. These tags correspond to labels that serve as identifiers describing particular data that are consequently categorised according to the assigned label. To promote better comprehension, referring to the example illustrated in Figure \ref{fig:named-entity-recognition-theoretical-example}, we have textual data in which labels are assigned as outlined in the following Table \ref{table:text-data-with-labels}.
\\
\begin{table}[ht]
    \centering
    \caption{Textual data with according labels.}
    \label{table:text-data-with-labels}
    \begin{tabular}[t]{ll}
        \hline
 Data&Label\\
        \hline
 Tim Cook&\textbf{PERSON}\\
 Apple Inc.&\textbf{ORG}ANISATION\\
 June 8th, 2023&\textbf{DATE}\\
        \hline
    \end{tabular}
\end{table}

\newpage

\begin{description}
    \item[Unsupervised learning] The unsupervised learning method discovers patterns of entity occurrence in raw and unlabeled data (see Figure \ref{fig:unsupervised-learning}). Consequently, the individual entities are split into groups based on their characteristic properties. The absence of pre-labeled data by human intervention in the training phase causes no supervisor to guide the model with information about the labels in training data, hence unsupervised learning. A conventional method employed in this approach is clustering, such as K-means clustering \parencite{9072123kmeans}, which divides data\footnote{In our case, the data corresponds to tokens symbolising words.} into groups based on similarity or dissimilarity.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{img/theoretical/unsupervised.pdf}
        \caption{Unsupervised learning approach.}
        \label{fig:unsupervised-learning}
    \end{figure}
    \item[Semi-supervised learning] The semi-supervised learning method combines labeled and unlabeled data, with the former comprising a slight portion of the dataset (see Figure \ref{fig:semi-supervised-learning}). As part of the classification training process, the unlabeled data learn the model's ability to generalise and represent the data in space using a statistical feature that better separates the classes. The algorithm aims to create the best decision boundary between classes based on a large amount of unlabeled data. Besides, the labeled data allows the model to determine the classification correctness for improvement, hence semi-supervised learning. Therefore, the model is partially supervised, hence semi-supervised learning. Semi-supervised learning is a preferred approach for model development because the labeled data is mainly expensive and time-consuming to acquire by requiring human intervention. In contrast, unlabeled data is more easily collectable.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth, interpolate=false]{img/theoretical/semi-supervised.pdf}
        \caption{Semi-supervised learning approach.}
        \label{fig:semi-supervised-learning}
    \end{figure}

    Self-training, Co-training, and Multi-view learning are frequently employed subcategories of this learning method. Self-training involves using a small amount of labeled data and a more significant amount of unlabeled data, repeatedly training the model on labeled data and using it to predict labels for unlabeled data. Co-training uses multiple independent models that communicate and share information to improve performance. Multi-view learning combines information from different sources or representations of data, such as combining textual data with metadata.
    \item[Supervised learning] Unlike the semi-supervised learning method, the supervised learning approach exclusively utilises labeled data. Therefore, the training process is performed just on labeled data (see Figure \ref{fig:supervised-learning}). We could perceive this approach as a function $f:~X~\mapsto~Y$, denoted as $f$, mapping input $X$ to output $Y$, where $X$ and $Y$ represent the input and output, respectively, known from the labeled data. Thus, as a supervisor guides the learning process, the model learns the most suitable way to map the input $X$ to the input $Y$. The principal contrast between fully and semi-supervised learning is that in the former case, learning is done only over labeled data and in the latter case on a combination with unlabeled data. The most common algorithms utilised in supervised learning for named entity recognition include statistical models like \acrfull{svm} \parencite{wang2005svm}, \acrfull{crf} \parencite{sutton2012crf}, \acrfull{me} \parencite{berger1996me}, and \acrfull{hmm} \parencite{EDDY1996361hmm}.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth, interpolate=false]{img/theoretical/supervised.pdf}
        \caption{Supervised learning approach.}
        \label{fig:supervised-learning}
    \end{figure}
    \item[Deep Learning] A given text could be extensive and contain many sentences or paragraphs mentioning entities in various forms. Therefore, deep learning comes into play due to its prowess in achieving better comprehension and learning contextual semantics. Its approaches overcome the primarily classification-focused methods introduced thus far, leading to state-of-the-art results in entity recognition. To enhance elucidation of the semantic context, methods' consideration of words occurring before and after words possess a role. The overarching structure of this approach's most commonly employed methods, such as \acrfull{rnn}, \acrfull{cnn}, and Transformer, is usually outlined into three fundamental stages: Data representation typically based on One-Hot Encoding, Word2Vec \parencite{mikolov2013word2vec}, \acrfull{tf-idf} \parencite{AIZAWA200345tfidf}, FastText \parencite{joulin2016bagfasttext}, \acrfull{glove} \parencite{pennington-etal-2014-glove}, or \acrfull{elmo} \parencite{peters2018elmo}Context encoding and Entity decoding.
\end{description}

